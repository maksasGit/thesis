\chapter{Rozdzial 2}
	
		
\section{Aplikacja}

	\subsection{Opis aplikacji}
		Ten projekt ma na celu stworzenie narzędzia do analizy danych tekstowych, obejmującego wstępne przetwarzanie tekstu, wektoryzację i klasteryzację. Głównym celem aplikacji jest umożliwienie użytkownikowi elastycznego dostosowania i wykorzystania różnych metod przetwarzania danych do rozwiązywania konkretnych zadań.
		
		
	\subsection{Technologie}
		Do realizacji projektu wybrano język programowania Python. Posiada on szereg zalet:
		\begin{itemize}
			\item Bogaty zestaw bibliotek do implementacji różnych metod, takich jak wstępne przetwarzanie tekstu, wektoryzacja i klasteryzacja.
			\item Wysoka efektywność w analizie danych dzięki bibliotekom takim jak NumPy, scikit-learn.
			\item Społeczność i dokumentacja, które ułatwiają rozwój i utrzymanie projektów.
		\end{itemize}
		
		
	\subsection{GUI}
		Główną ideą aplikacji jest stworzenie wygodnego narzędzia do analizy danych tekstowych, w którym użytkownik może samodzielnie wybierać i dostosowywać metody przetwarzania do konkretnych zadań za pomocą interfejsu graficznego. Do opracowania interfejsu graficznego wykorzystano biblioteki PyQt i Matplotlib:
		\begin{itemize}
			\item PyQt: Umożliwia tworzenie intuicyjnych interfejsów użytkownika.
			\item Matplotlib: Wykorzystywana do wizualizacji danych, co pozwala użytkownikowi łatwo interpretować wyniki analizy.
		\end{itemize}
		Interfejs graficzny umożliwia użytkownikowi ładowanie danych, konfigurowanie parametrów przetwarzania tekstu, wektoryzacji i klasteryzacji oraz wizualizację wyników.
		
		
		
	\subsection{OOP}
		Głównym komponentem mojej aplikacji jest klasa \textbf{Workspace}, która stanowi kontener przechowujący parametry dla trzech głównych procesów:
		\begin{itemize}
			\item Przetwarzanie tekstu: Obejmuje metody wstępnego przetwarzania, takie jak tokenizacja, usuwanie stop-słów i lematyzacja.
			\item Wektoryzacja: Przekształca dane tekstowe w wektory liczbowe przy użyciu metod takich jak TF-IDF lub Word2Vec.
			\item Klasteryzacja: Implementuje algorytmy, takie jak K-means lub DBSCAN, do grupowania danych tekstowych.
		\end{itemize}
		Klasa \textbf{Workspace} zapewnia strukturalne przechowywanie i zarządzanie parametrami każdego etapu, co upraszcza proces konfiguracji i wykonania analizy.
		
	
\section{Dane tekstowe}

	\subsection{Opis}
		Aby zrealizować ten projekt, potrzebne będą dane tekstowe, które w tym przypadku będą komentarzami z YouTube. Do pozyskiwania informacji o filmach i powiązanych komentarzach będziemy używać YouTube Data API v3. API to jest darmowe i łatwe w użyciu. Aby rozpocząć pracę, należy zarejestrować się i utworzyć projekt w Google Cloud, a następnie aktywować YouTube Data API. Po tym wygenerowany zostanie klucz API, który będziemy używać do uwierzytelniania.
		
	\subsection{Zapytania API}
		W projekcie wykorzystane będą dwa zapytania HTTP:
		\begin{itemize}
			\item "GET https://www.googleapis.com/youtube/v3/commentThreads"
			\item "GET https://www.googleapis.com/youtube/v3/videos"
		\end{itemize}
	 	Pierwsze zapytanie służy do pobierania komentarzy do konkretnego filmu, a drugie do uzyskiwania informacji o filmie. Szczegółowe informacje można znaleźć w dokumentacji \cite{YouTubeAPI}. Etapy pobierania komentarzy:
	 	\begin{enumerate}
	 		\item Użytkownik wprowadza w interfejsie graficznym (GUI) link do interesującego go filmu.
	 		\item Program wyodrębnia \textbf{video\_id} z podanego linku.
	 		\item Za pomocą \textbf{video\_id} wykonywane jest zapytanie GET https://www.googleapis.com/youtube/v3/videos, aby uzyskać podstawowe informacje o filmie. To pozwala użytkownikowi upewnić się, że wybrany film odpowiada jego oczekiwaniom.
	 		\item Po potwierdzeniu przez użytkownika, że film jest prawidłowy, wykonywane jest zapytanie GET https://www.googleapis.com/youtube/v3/commentThreads, aby pobrać komentarze do tego filmu.
	 		\item Uzyskane komentarze są zapisywane w pliku tekstowym, gdzie każdy komentarz jest zapisany w oddzielnej linii. Nazwa pliku odpowiada unikalnemu identyfikatorowi filmu (\textbf{video\_id}).
	 	\end{enumerate}
	
	\subsection{Przetwarzanie uzyskanych danych}
		Wyniki naszych zapytań będą dostarczane w formacie JSON. Szczegółowe informacje o strukturze odpowiedzi JSON można znaleźć w dokumentacji \cite{YouTubeAPI}. Do uzyskiwania informacji o filmie będą używane tylko następujące pola:
		\begin{itemize}
			\item title: tytuł filmu
			\item channelTitle: nazwa autora kanału
			\item thumbnails: URL miniaturki
		\end{itemize}				
		 Przy pobieraniu komentarzy potrzebne są tylko następujące pola:
		 \begin{itemize}
		 	\item textDisplay: treść komentarza
		 	\item replies: kontener z odpowiedziami na komentarze
		 \end{itemize}
 		 Pole \textbf{textDisplay} zawiera tekst komentarza, a pole \textbf{replies} jest kontenerem na odpowiedzi na komentarze. Każda odpowiedź w \textbf{replies.comments} również ma pole \textbf{textDisplay}, które możemy osobno przetwarzać. W końcowym wyniku otrzymamy listę wszystkich komentarzy, gdzie każdy element reprezentuje oddzielny komentarz.
		
	\subsection{Przechowywanie}
		Po uzyskaniu wszystkie komentarze będą przechowywane w osobnym katalogu \textbf{comments}, który zostanie utworzony w przypadku jego braku. Każdy plik z komentarzami będzie miał nazwę \textbf{video\_id.txt} dla łatwej identyfikacji.
		Dla uproszczenia procesu przechowywania i dostępu do danych oraz zapewnienia ich kompatybilności z różnymi narzędziami analitycznymi, zdecydowano się na użycie zwykłego pliku tekstowego (.txt) do przechowywania komentarzy. Każdy komentarz będzie zapisany w oddzielnej linii pliku, co upraszcza strukturę danych i umożliwia łatwe ich odczytanie do analizy.
		Taki sposób przechowywania danych został wybrany z kilku powodów:
	\begin{itemize}
		\item \textbf{Prostota implementacji}: Pliki tekstowe łatwo tworzyć, czytać i edytować za pomocą wielu narzędzi programistycznych i języków programowania.
		\item \textbf{Kompatybilność}: Praktycznie wszystkie narzędzia analityczne i biblioteki mogą pracować z plikami tekstowymi, co ułatwia integrację danych w różnych etapach analizy.
		\item \textbf{Mały wpływ na analizę}: Format przechowywania nie ma znaczącego wpływu na sam proces analizy danych. Główne znaczenie ma zawartość komentarzy, a nie sposób ich przechowywania.
	\end{itemize}

\section{Przetwarzania tekstu}

	\subsection{Defenicja}
		Następnym procesem jest przetwarzanie tekstu. Ważne jest, aby zrozumieć, jak wyglądają komentarze po ich pobraniu i zapisaniu z YouTube. Implementacja przetwarzania zostanie wykonana za pomocą biblioteki NLTK (Natural Language Toolkit) oraz wbudowanych funkcji Pythona. Dokumentację NLTK można znaleźć tutaj \cite{nltk}. Przetwarzanie tekstu jest podzielone na kilka etapów:
		\begin{itemize}
			\item Przetwarzanie obowiązkowe, które jest wykonywane niezależnie od wybranych parametrów przez użytkownika.
			\item Przetwarzanie prymitywne, czyli usuwanie symboli i cyfr.
			\item Usuwanie stop-words.
			\item Lematyzacja.
			\item Analiza.
		\end{itemize}
		Wszystkie metody, oprócz przetwarzania obowiązkowego i analizy, są opcjonalne, co oznacza, że będą wykonywane tylko wtedy, gdy użytkownik wskaże, że chce używać tych metod.
		
	\subsection{Przetwarzanie obowiązkowe}
		Obowiązkowe etapy przetwarzania tekstu to zmiana wszystkich słów na małe litery oraz proces tokenizacji, który eliminuje potrzebę usuwania spacji. Za pomocą funkcji \textbf{nltk.word\_tokenize} każde słowo zostanie podzielone na tokeny. Te etapy są obowiązkowe, ponieważ upraszczają dalszą pracę z tekstem i gwarantują poprawne działanie wielu późniejszych funkcji, które mogą być czułe na wielkość liter.
	
		
	\subsection{Przetwarzanie prymitywne}
		Przetwarzanie prymitywne polega na usuwaniu niealfabetycznych symboli lub słów. Ten proces nie jest obowiązkowy, jednak jego zastosowanie pomaga pozbyć się różnych języków znaczników, które YouTube używa do wizualizacji emotikonów lub narzędzi do zmiany tekstu.

		Przykłady takich komentarzy:

		\begin{verbatim}
			"Great video! (tu musi byc emoji, ale nie wiem jak tu wkleic)"
			"Check this out: <a href='https://example.com'>link</a>"
		\end{verbatim}
		W tym projekcie jest to realizowane za pomocą wbudowanej funkcji Pythona \textbf{isalpha()}, która sprawdza, czy symbole lub słowa zawierają niealfabetyczne znaki.
		\begin{lstlisting}[language=Python]
			def remove_non_alpha(tokens):
			return [word for word in tokens if word.isalpha()]
		\end{lstlisting}
		
	\subsection{Usuwanie stop-words}
		Ten etap polega na usuwaniu słów, które nie niosą znaczącej informacji. Typowymi przykładami takich słów są:
		\begin{verbatim}
			"the", "is", "in", "and"
		\end{verbatim}
		Usuwanie stop-words zmniejsza rozmiar tekstu i sprawia, że znaczące słowa są bardziej widoczne. Do tego celu używa się słownika stop-words z biblioteki NLTK, który w razie potrzeby można uzupełnić własnymi słowami.
		\begin{lstlisting}[language=Python]
		from nltk.corpus import stopwords

		def remove_stop_words(tokens):
			stop_words = set(stopwords.words('english'))
			return [word for word in tokens if word not in stop_words]
		\end{lstlisting}

	\subsection{Lematyzacja}
		Ostatnim opcjonalnym procesem jest lematyzacja, która sprowadza słowa do ich podstawowej formy. Jest to przydatne w sytuacjach, gdy trzeba zgrupować różne formy jednego słowa (np. "running", "ran", "runs" do "run").

		\begin{lstlisting}[language=Python]
		from nltk.stem import WordNetLemmatizer

		def lemmatize_tokens(tokens):
			lemmatizer = WordNetLemmatizer()
		return [lemmatizer.lemmatize(word) for word in tokens]
		\end{lstlisting}

	
	\subsection{Analiza}
		Na końcu, niezależnie od wybranych przez użytkownika etapów, wszystkie tokeny są sprawdzane pod kątem niepustych wartości, aby uniknąć całkowicie pustych słów i wierszy. Następnie wszystkie tokeny są łączone w jeden ciąg znaków, oddzielony spacjami. Na tym etapie wynik przetwarzania jest zwracany i analizowany za pomocą funkcji analizy częstotliwości (\textbf{freq\_analysis}) i analizy długości (\textbf{length\_analysis}). Te funkcje będą używane do tworzenia wykresów, które pomogą użytkownikowi wizualnie ocenić częstotliwość występowania słów i długość komentarzy w celu podejmowania dalszych decyzji.
	
		\subsubsection{Analiza częstotliwości (freq analysis)}
			Ta funkcja używa przetworzonych ciągów znaków, tworząc pojedynczą tablicę ze wszystkimi słowami ze wszystkich komentarzy, i stosuje klasę \textbf{collections.Counter} w Pythonie. \textbf{Counter} jest używany do zliczania hashowalnych obiektów, takich jak ciągi znaków lub liczby, i dostarcza wygodne metody do pracy z danymi częstotliwości. W ten sposób funkcja generuje dwie tablice: tablicę słów i odpowiadającą jej tablicę liczby wystąpień każdego słowa w tekście. Te tablice będą używane do tworzenia wykresu słupkowego (bar chart), pokazującego częstotliwość występowania słów.
			
			Przykład implementacji algorytmu w projekcie:
			\begin{lstlisting}[language=Python]
				from collections import Counter
				import matplotlib.pyplot as plt

				def freq_analysis(tokens):
					data = ' '.join(self.nlp_data)
					words = nltk.word_tokenize(data)
					# words = [word.lower() for word in words]
					# words = [word for word in words if word.isalnum()]
					word_freq = Counter(words)
					words_counts_sorted = word_freq.most_common()
					self.words_sorted = [word_count[0] for word_count in words_counts_sorted]
					self.counts_sorted = [word_count[1] for word_count in words_counts_sorted]
			\end{lstlisting}
		
		\subsubsection{Analiza długości (length analysis)}
			Ta funkcja również używa przetworzonych ciągów znaków komentarzy, określając liczbę słów w każdym komentarzu. Ostatecznie generowana jest tablica z liczbą słów, która będzie używana do tworzenia histogramu (histogram), pokazującego rozkład długości komentarzy.
			
			Przykład implementacji algorytmu w projekcie:
			\begin{lstlisting}[language=Python]
				from collections import Counter
				import matplotlib.pyplot as plt
				
				def freq_analysis(tokens):
					word_counts = [len(nltk.word_tokenize(el)) for el in self.nlp_data]
					self.max_size = max(word_counts) + 1
					self.lengths = [0] * (self.max_size + 1)
					for count in word_counts:
					self.lengths[count] += 1
					endTime = time.time()
			\end{lstlisting}
	
	\subsection{Podsumowanie}
		Wszystkie etapy przetwarzania tekstu, od obowiązkowych do opcjonalnych, odgrywają ważną rolę w przygotowaniu danych do dalszej analizy. Ich poprawne wykonanie gwarantuje strukturalizowany i wygodny do przetwarzania wygląd danych. Wizualizacja wykresów analizujących zawartość komentarzy pozwoli użytkownikom lepiej zrozumieć strukturę i treść komentarzy, a także podejmować uzasadnione decyzje na podstawie przedstawionych danych.
	
	
\section{Wektoryzacja}
	
	\subsection{Defenicja}
		Na tym etapie tekst jest gotowy do dalszego przetwarzania. Aby przeprowadzić analizę, konieczne jest przekształcenie komentarzy w formę wektorową (wektoryzacja). Zostaną użyte dwie metody: TF-IDF i Word2Vec. Te metody umożliwiają przedstawienie danych tekstowych w postaci wektorów liczbowych, co ułatwia dalszą analizę i przetwarzanie. Wektoryzacja będzie realizowana przy użyciu bibliotek \textbf{scikit-learn} \cite{scikit-learn} dla metody TF-IDF i \textbf{gensim} \cite{gensim} dla metody Word2Vec. 

		\underline{Moje pytanie}\\
		\textbf{Tu, jak i w następnych sekcjach mam pytania, bo nie wiem, jak opisać dlaczego wybrałem dokładnie te metody. Wybrałem tak naprawdę dwie najpopularniejsze metody, i wybrałem dwie, bo chciałem je porównać pod względem wyników.}

	
	\subsection{TF-IDF}
 		TF-IDF (Term Frequency-Inverse Document Frequency) to miara statystyczna używana do oceny ważności terminu w dokumencie w odniesieniu do kolekcji dokumentów (korpusu). Jest szeroko stosowana w zadaniach przetwarzania języka naturalnego i wyszukiwania informacji. W bibliotece Scikit-learn istnieje wygodne narzędzie do obliczania TF-IDF: klasa TfidfVectorizer. \cite{tf-idf}
		
		
		\subsubsection{Parametry}
			\begin{itemize}
				\item max\_df: Liczba lub udział. Terminy, które występują w większej liczbie dokumentów niż podano, są ignorowane. Przydatne do usuwania powszechnie używanych słów.
				\item min\_df: Liczba lub udział. Terminy, które występują w mniejszej liczbie dokumentów niż podano, są ignorowane. Przydatne do usuwania rzadkich słów.
				\item ngra\_range: Krotka (min\_n, max\_n). Określa zakres dla ekstrakcji n-gramów.
				\item stop\_words: Lista słów lub ciąg znaków określający słowa, które należy ignorować.
			\end{itemize}
		
		\subsubsection{Wyjaśnienie algorytmu}
			\begin{itemize}
				\item Term Frequency (TF)
				\item Inverse Document Frequency (IDF)
				\item TF-IDF
			\end{itemize}
			
				
		\subsubsection{Przykład użycia}
			\begin{verbatim}
				from sklearn.feature_extraction.text import TfidfVectorizer
	
				def vectorize_tfidf(corpus):
					vectorizer = TfidfVectorizer()
					tfidf_matrix = vectorizer.fit_transform(corpus)
				return tfidf_matrix
			\end{verbatim}

			Przykład przekształcenia danych tekstowych:	
			\begin{itemize}
				\item Przed: ["This is a great video", "I love this content"]
				\item Po: [[0.5, 0.5, 0, 0.5, 0, 0.5], [0, 0, 0.7, 0, 0.7, 0]]
			\end{itemize}
			
			Aby korzystać z TF-IDF, potrzebny jest korpus tekstów, który można przedstawić jako listę ciągów znaków, gdzie każdy ciąg znaków to osobny dokument.
			
 
	\subsection{Word2Vec}
		Word2Vec to algorytm opracowany do nauki wektorowych reprezentacji słów (embeddingów), które oddają semantyczne relacje między słowami. W bibliotece Gensim dostępny jest wygodny interfejs do pracy z Word2Vec. \cite{word2vec}


		\subsubsection{Parametry}	
			\begin{itemize}
				\item sentences: Korpus tekstów, przedstawiony jako lista list słów.
				\item vector\_size: Wymiarowość wektorowej reprezentacji słów.
				\item window: Maksymalna odległość między bieżącym a przewidywanym słowem w zdaniu.
				\item min\_count: Minimalna częstotliwość słowa w korpusie, aby je uwzględnić.
				\item workers: Liczba wątków do równoległego uczenia.
			\end{itemize}
		
			Dodatkowe ustawienia:
			\begin{itemize}
				\item SG (Skip-Gram) i CBOW (Continuous Bag of Words): W Word2Vec są dwa główne podejścia architektoniczne — Skip-Gram i CBOW. Parametr sg w Gensim pozwala wybrać między nimi:
					\begin{itemize}
						\item sg=0 używa CBOW (domyślnie).
						\item sg=1 używa Skip-Gram.
					\end{itemize}
				\item Negative Sampling: W Word2Vec używa się negatywnego próbkowania dla zwiększenia efektywności uczenia. Parametr negative określa liczbę "negatywnych" przykładów dla każdego pozytywnego przykładu.
				\item Learning Rate: Parametr alpha określa początkową szybkość uczenia, a min\_alpha — minimalną szybkość uczenia, która zmniejsza się do tej wartości w trakcie uczenia..
			\end{itemize}
		
		\subsubsection{Opis algorytmu}
			\begin{enumerate}
				\item Inicjalizacja wag: Wagi sieci neuronowej są inicjalizowane losowymi wartościami. Są to wagi między warstwą wejściową a ukrytą, oraz między warstwą ukrytą a wyjściową.
				\item Przejście do przodu (Forward Pass): Dla każdego słowa w zdaniu tworzony jest wektor wejściowy. W przypadku Skip-Gram ten wektor wejściowy jest używany do przewidywania słów kontekstowych. W przypadku CBOW słowa kontekstowe są używane do przewidywania bieżącego słowa.
				\begin{itemize}
					\item Skip-Gram:
					\begin{itemize}
						\item Słowo wejściowe jest kodowane jako jednowymiarowy wektor.
						\item Jednowymiarowy wektor jest mnożony przez wagi między warstwą wejściową a ukrytą, aby uzyskać ukrytą reprezentację.
						\item Ukryta reprezentacja jest mnożona przez wagi między warstwą ukrytą a wyjściową, aby przewidzieć słowa kontekstowe.
					\end{itemize}
					\item CBOW:
					\begin{itemize}
						\item Słowa kontekstowe są kodowane jako jednowymiarowe wektory.
						\item Jednowymiarowe wektory są sumowane i mnożone przez wagi między warstwą wejściową a ukrytą, aby uzyskać ukrytą reprezentację.
						\item Ukryta reprezentacja jest mnożona przez wagi między warstwą ukrytą a wyjściową, aby przewidzieć bieżące słowo.
					\end{itemize}								
				\end{itemize}
				\item Propagacja wsteczna (Backpropagation): Błąd przewidywania jest obliczany przez porównanie przewidywanego słowa z faktycznym. Następnie ten błąd jest propagowany wstecz przez sieć, aby zaktualizować wagi. Używa się metody stochastycznego spadku gradientu (SGD) i, w niektórych przypadkach, negatywnego próbkowania, aby przyspieszyć uczenie.
				\begin{itemize}
					\item Aktualizacja wag: Wagi są aktualizowane w kierunku przeciwnym do gradientu błędu. To pomaga zminimalizować błąd przewidywania.
				\end{itemize}
				\item Powtarzanie procesu: Proces ten powtarza się dla wszystkich słów w zdaniu i dla wszystkich zdań w korpusie. Model przechodzi przez korpus kilka razy, zgodnie z wartością parametru epochs.
			\end{enumerate}
	
	
 		\subsubsection{Przykład użycia}
 			\begin{verbatim}
				from gensim.models import Word2Vec
	
				def vectorize_word2vec(tokens):
				model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4)
				word_vectors = model.wv
				return word_vectors
			\end{verbatim}

			Przykład przekształcenia danych tekstowych:

			\begin{itemize}
				\item Przed: ["This is a great video", "I love this content"]
				\item Po: Wektory słów: { "this": [...], "is": [...], "a": [...], "great": [...], "video": [...], "I": [...], "love": [...], "content": [...] }
			\end{itemize}
 	
 	\subsection{Ocena}
 		Po wektoryzacji tekstu za pomocą metod TF-IDF i Word2Vec ważne jest przeprowadzenie oceny ich skuteczności i zrozumienie, jak dobrze radzą sobie z zadaniem przedstawiania danych tekstowych. W tej sekcji zostaną omówione kryteria oceny i wyniki zastosowania każdej z metod na naszych danych.
 	
 		\subsubsection{Kryteria}
 			Do oceny pracy metod TF-IDF i Word2Vec używane są następujące kryteria:
 		
			\begin{itemize}
				\item \textbf{Znaczenie semantyczne:} Jak dobrze metoda uchwytuje semantyczne związki między słowami i kontekstem ich użycia.
				\item \textbf{Szybkość obliczeń:} Czas potrzebny na wektoryzację tekstów.
				\item \textbf{Łatwość interpretacji:} Jak łatwo interpretować wyniki wektoryzacji i używać ich w dalszych analizach.
				\item \textbf{Zastosowalność do zadań uczenia maszynowego:} Jak dobrze wektory nadają się do zadań klasyfikacji, klasteryzacji i innych metod uczenia maszynowego.
			\end{itemize}
		
		\subsubsection{Ocena  TF-IDF}
			\textit{Tego nie mam w kodzie, moze to i nie jest potrzebne}
		
		\subsubsection{Ocena  Word2Vec}
			\textit{Jest nie wielka czesc, ale napisano to bylo przez chatGPT, zeby ja mog sprawdzic}
		
	
\section{Klasteryzacja}

	\subsection{Defenicja}
		Kolejnym etapem przetwarzania danych jest klasteryzacja, która wykorzystuje zwektoryzowane komentarze i dzieli je na klastry. Zostanie to zrealizowane za pomocą dwóch metod: K-means i DBSCAN, z wykorzystaniem biblioteki sklearn \cite{scikit-learn}.  \cite{dbscan}
	
	\subsection{K-means}
 		Algorytm K-means dzieli zbiór danych na nieprzekrywające się klastry, z których każdy opisany jest średnią wartością (centroidem) próbek w klastrze. K-means dąży do wyboru centroidów, minimalizując bezwładność, czyli kryterium sumy kwadratów wewnątrz klastrów \cite{k-means}.
	
		\subsubsection{Описание алгоритма}
			Algorytm K-means można zrozumieć przez trzy główne kroki. Pierwszy krok polega na wyborze początkowych centroidów. Zwykle używa się prostego metody losowego wyboru próbek z zestawu danych. Po inicjalizacji K-means wykonuje dwa kolejne kroki. Pierwszy krok polega na przypisaniu każdej próbki do najbliższego centroidu. Drugi krok tworzy nowe centroidy, obliczając średnią wartość wszystkich próbek przypisanych do każdego z poprzednich centroidów. Różnica między starymi a nowymi centroidami jest obliczana i algorytm powtarza te dwa ostatnie kroki, aż ta różnica nie będzie mniejsza od określonego progu. Innymi słowy, algorytm powtarza się, dopóki centroidy nie przestaną się znacząco przesuwać.

			Jak wiele algorytmów, K-means jest podatny na problem lokalnych minimów, co silnie zależy od początkowej inicjalizacji centroidów. Zazwyczaj algorytm uruchamia się kilkakrotnie z różnymi inicjalizacjami centroidów.


			
		\subsubsection{Parametry}
			Do konfiguracji algorytmu K-means używa się następujących parametrów:
			\begin{itemize}
				\item n\_clusters: Liczba klastrów (K), na które należy podzielić dane.
				\item init: Metoda inicjalizacji centroidów.
				\item max\_iter: Maksymalna liczba iteracji dla jednego uruchomienia K-means.
				\item tol: Próg dla zatrzymania algorytmu. Jeśli różnica między starymi a nowymi centroidami jest mniejsza od tej wartości, algorytm zatrzymuje się.
				\item n\_init: Liczba uruchomień algorytmu z różnymi inicjalizacjami. Wynik z najmniejszą bezwładnością zostanie wybrany jako ostateczny.
				\item random\_state: Wartość początkowa generatora liczb losowych dla powtarzalności wyników.
				\item algorithm: Algorytm do obliczania K-means. Możliwe wartości to 'auto', 'full' i 'elkan'.
			\end{itemize}
			
		\subsubsection{Przykład użycia}
			\begin{verbatim}
				kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10, random_state=0)


				kmeans.fit(X)


				y_kmeans = kmeans.predict(X)
			\end{verbatim}
			
			
			Przykład przekształcenia danych:
			\underline{tu przyklad}

	  
	\subsection{DBSCAN}
		Algorytm DBSCAN (Density-Based Spatial Clustering of Applications with Noise) traktuje klastry jako obszary o wysokiej gęstości, oddzielone od obszarów o niskiej gęstości. Dzięki temu DBSCAN może wykrywać klastry o dowolnym kształcie, w przeciwieństwie do K-means, który zakłada, że klastry mają kształt wypukły. Głównym elementem DBSCAN jest pojęcie punktów rdzeniowych, które reprezentują próbki w obszarach o wysokiej gęstości. \cite{dbscan}


		\subsubsection{Parametry}
			Parametry algorytmu DBSCAN:
			\begin{itemize}
				\item eps: Maksymalna odległość między dwiema próbkami, przy której jedna jest uważana za sąsiada drugiej. Jest to parametr promienia do określania gęstości obszaru.
				\item min\_samples: Minimalna liczba próbek (w tym sama próbka) potrzebna do uznania obszaru za gęsty, tj. do uznania próbki za rdzeniową.
				\item metric: Metryka do obliczania odległości między punktami (domyślnie 'euclidean'). Może to być dowolna metryka obsługiwana przez funkcję scipy.spatial.distance.pdist.
				\item algorithm: Algorytm używany do obliczania najbliższych sąsiadów ('auto', 'ball\_tree', 'kd\_tree', 'brute').
				\item leaf\_size: Rozmiar liścia przekazywany algorytmom 'ball\_tree' i 'kd\_tree'. Może to wpłynąć na szybkość budowy i zapytania najbliższych sąsiadów.
				\item p: Wykładnik odległości Minkowskiego, gdy używana jest metryka 'minkowski' (domyślnie p=2, co odpowiada odległości euklidesowej).
			\end{itemize}
			
			
		\subsubsection{Opis algorytmu}
			Algorytm DBSCAN działa w następujący sposób:
			\begin{enumerate}
				\item Określenie punktów rdzeniowych: Próbki, dla których liczba sąsiadów w obrębie eps nie jest mniejsza niż min\_samples.
				\item Tworzenie klastrów: Dla każdej próbki rdzeniowej tworzony jest klaster poprzez znalezienie wszystkich jej sąsiadów, którzy również są próbkami rdzeniowymi, oraz ich własnych sąsiadów itd. Każdy klaster obejmuje także próbki nie będące rdzeniowymi, które są sąsiadami próbek rdzeniowych w klastrze.
				\item Wykrywanie szumów: Wszystkie próbki, które nie są rdzeniowe i nie znajdują się w wystarczającej odległości od próbek rdzeniowych, są uznawane za szumy.
			\end{enumerate}
			Algorytm DBSCAN jest deterministyczny i zawsze generuje takie same klastry dla tych samych danych wejściowych w tej samej kolejności. Jednak wyniki mogą się różnić przy podaniu danych w innej kolejności. Wynika to z faktu, że kolejność przetwarzania danych wpływa na sposób, w jaki próbki są przypisywane do klastrów.

			
		\subsubsection{Przykład użycia}
			\begin{verbatim}
				X = np.array([[0, 1], [1, 1], [2, 1], [3, 1], [4, 1],
             			 [0, 0], [1, 0], [2, 0], [3, 0], [4, 0]])


					tsne = TSNE(n_components=2, perplexity=5, n_iter=300, random_state=0)


					X_embedded = tsne.fit_transform(X)
			\end{verbatim}
			
			
			Przykład przekształcenia danych:
				\underline{tu przyklad}
		
	\subsection{Dobór parametrów}
		Dobór odpowiednich parametrów dla algorytmów klasteryzacji, takich jak K-means i DBSCAN, jest kluczowy dla uzyskania satysfakcjonujących wyników. Poniżej przedstawione są niektóre techniki i wskazówki dotyczące doboru parametrów:
		
		\begin{itemize}
			\item K-means: Ważnym parametrem jest liczba klastrów (n\_clusters). Można użyć metody "łokcia" (elbow method) lub średniego współczynnika sylwetki (silhouette score) do określenia optymalnej liczby klastrów. Parametry takie jak max\_iter, tol oraz n\_init mogą być dostosowane w celu poprawy zbieżności i stabilności algorytmu.
			\item DBSCAN: Kluczowymi parametrami są eps i min\_samples. Wybór eps zależy od gęstości danych i można go oszacować na podstawie wykresu odległości K-najbliższych sąsiadów (K-distance plot). Parametr min\_samples można ustawić w zależności od oczekiwanej minimalnej liczby punktów w klastrze.
		\end{itemize}
		
		\underline{this is new}
			
	
	
\section{Wizualizacja}
	\subsection{Defenicja}
		Ostatnim etapem analizy jest wizualizacja danych. Końcowy wynik to wykres 2D, gdzie każdy punkt będzie reprezentował pewne dane tekstowe (komentarze) z określonym kolorem odpowiadającym klastrowi, do którego należy, lub kolorem braku klastra, jak w przypadku DBSCAN. Na wykresie będą również wizualizowane centroidy każdego klastra w postaci czerwonego krzyżyka.
	
	\subsection{Redukcja wymiarowości}
		Aby możliwa była wizualizacja zwektoryzowanych danych jako punktów 2D, konieczne jest skorzystanie z metody redukcji wymiarowości (T-SNE). Ta metoda jest nieliniową metodą ekstrakcji cech. Na początku trudno jest określić, za co odpowiadają cechy, więc wybór odpowiednich cech jest niemożliwy. Metoda nieliniowa pomoże nam zachować nieliniowe zależności \cite{t-sne}.
	
		
	\subsection{T-SNE}
	
		\subsubsection{Parametry}
			\begin{itemize}
				\item n\_components: Liczba wymiarów, na które należy zrzutować dane (zwykle 2 lub 3 dla wizualizacji).
				\item perplexity: Parametr związany z liczbą najbliższych sąsiadów. Wpływa na równowagę między lokalną a globalną strukturą danych.
				\item early\_exaggeration: Parametr kontrolujący odległości między punktami na początkowym etapie optymalizacji.
				\item learning\_rate: Szybkość uczenia się dla spadku gradientowego.
				\item n\_iter: Liczba iteracji do optymalizacji.
				\item metric: Metryka do obliczania odległości między punktami (domyślnie 'euclidean').
				\item init: Metoda inicjalizacji ('random' lub 'pca').			
				\item random\_state: Początkowa wartość generatora liczb losowych dla reprodukowalności wyników.
			\end{itemize}				
		
		\subsubsection{Opis algorytmu}
			Algorytm t-SNE działa w następujący sposób: 
			\begin{enumerate}
				\item Przekształcenie odległości na prawdopodobieństwa: Dla każdego punktu obliczane są prawdopodobieństwa Pearsona, które określają prawdopodobieństwo wybrania sąsiedniego punktu jako sąsiada.
				\item Określenie docelowych prawdopodobieństw: Na niskowymiarowej projekcji obliczane są prawdopodobieństwa Q, podobne do prawdopodobieństw Pearsona, ale w innym wymiarze.
				\item Minimalizacja rozbieżności (Kullback-Leibler divergence): Używany jest spadek gradientowy do minimalizacji rozbieżności między rozkładami P i Q, co prowadzi do zachowania bliskości punktów.
			\end{enumerate}
			
			
		\subsubsection{Przykład użycia}
			\begin{verbatim}
				I use T-SNE
			\end{verbatim}
			
			
			Przykład przekształcenia danych:
			
				\textbf{tu będzie zdjęcie}
	
	






