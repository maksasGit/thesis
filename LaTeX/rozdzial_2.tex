\chapter{Rozdzial 2}
	
	
	\begin{figure}
		\centering
		\scalebox{0.25} {\includegraphics[width=0.5\textwidth]{./img/rys1.jpg	}}
		\caption{Przyklad rys}
		\label{fig:rys1}
	\end{figure}
		

\section{Приложение}

	\subsection{Opis}
		Данный проект направлен на создание инструмента для анализа текстовых данных, включающего предобработку текста, векторизацию и кластеризацию. Основная цель приложения — предоставить пользователю возможность гибкой настройки и использования различных методов обработки данных для решения конкретных задач.
		
	\subsection{Технологии}
		Для реализации проекта был выбран язык программирования Python. Он обладает рядом преимуществ:
		\begin{itemize}
			\item Богатый набор библиотек для реализации различных методов, таких как предобработка текста, векторизация и кластеризация.
			\item Высокая эффективность для анализа данных благодаря библиотекам, таким как NumPy, scikit-learn.
			\item Сообщество и документация, которые облегчают разработку и поддержку проектов.
		\end{itemize}
		
		
	\subsection{GUI}
		Основная идея приложения заключается в создании удобного инструмента для анализа текстовых данных, где пользователь может самостоятельно выбирать и настраивать методы обработки под конкретные задачи с помощью графического интерфейса. Для разработки графического интерфейса были использованы библиотеки PyQt и Matplotlib:
		\begin{itemize}
			\item PyQt: Обеспечивает создание интуитивно понятных пользовательских интерфейсов.
			\item Matplotlib: Используется для визуализации данных, что позволяет пользователю легко интерпретировать результаты анализа.
		\end{itemize}
		Графический интерфейс позволяет пользователю загружать данные, настраивать параметры обработки текста, векторизации и кластеризации, а также визуализировать результаты.
		
	\subsection{OOP}
		Главным компонентом моего приложения является класс \textbf{Workspace}, который представляет собой контейнер, хранящий параметры для трех основных процессов:
		\begin{itemize}
			\item Обработка текста: Включает методы предобработки, такие как токенизация, удаление стоп-слов и лемматизация.
			\item Векторизация: Преобразует текстовые данные в числовые векторы с использованием методов, таких как TF-IDF или Word2Vec.
			\item Кластеризация: Реализует алгоритмы, такие как K-means или DBSCAN, для группировки текстовых данных.
		\end{itemize}
		Класс \textbf{Workspace} обеспечивает структурированное хранение и управление параметрами каждого этапа, что упрощает процесс настройки и выполнения анализа.
		
	
\section{Текстовые данные}

	\subsection{Opis}
		Для реализации данного проекта потребуются текстовые данные, которые в данном случае будут комментариями с YouTube. Мы будем использовать YouTube Data API v3 для получения информации о видео и связанных с ним комментариев. Данное API является бесплатным и простым в использовании. Для начала работы необходимо зарегистрироваться и создать проект в Google Cloud, а затем активировать YouTube Data API. После этого будет сгенерирован API-ключ, который мы будем использовать для аутентификации.
		
	\subsection{API запросы}
		В данном проекте будут использованы два HTTP-запроса:
		\begin{itemize}
			\item "GET https://www.googleapis.com/youtube/v3/commentThreads"
			\item "GET https://www.googleapis.com/youtube/v3/videos"
		\end{itemize}
	 	Первый запрос помогает получить комментарии к конкретному видео, а второй — получить информацию о видео. Подрбобно ознакомиться с полной документацией можно в \cite{YouTubeAPI}.
	 	 Этапы получения комментариев: 
	 	\begin{enumerate}
	 		\item Пользователь с помощью графического интерфейса (GUI) вводит ссылку на интересующее его видео.
	 		\item Программа извлекает video\_id из предоставленной ссылки.
	 		\item Используя video\_id, выполняется запрос GET https://www.googleapis.com/youtube/v3/videos для получения краткой информации о видео. Это позволяет пользователю убедиться, что выбранное видео соответствует его ожиданиям.
	 		\item После подтверждения пользователем, что видео корректное, выполняется запрос GET https://www.googleapis.com/youtube/v3/commentThreads, чтобы получить комментарии к этому видео.
	 		\item Полученные комментарии сохраняются в текстовый файл, где каждый комментарий записан в отдельной строке. Название файла соответствует уникальному идентификатору видео (video\_id).
	 	\end{enumerate}
	
	\subsection{Обработка полученных данных}
		Результаты наших запросов будут предоставлены в формате JSON. Подробную информацию о структуре JSON-ответов можно найти в документации \cite{YouTubeAPI}. Для получения информации о видео будут использоваться только следующие поля:
		\begin{itemize}
			\item title: название видеоролика
			\item channelTitle: имя автора канала
			\item thumbnails: URL-адрес изображения
		\end{itemize}				
		 При получении комментариев необходимы только следующие поля:
		 \begin{itemize}
		 	\item textDisplay: содержание комментария
		 	\item replies: контейнер комментариев-ответов
		 \end{itemize}
 		 Поле textDisplay содержит текст комментария, а поле replies является контейнером для комментариев-ответов. Каждый ответ в поле replies.comments также имеет поле textDisplay, которое мы можем отдельно обработать. 
		В конечном итоге мы получим список всех комментариев, где каждый элемент представляет из себя отдельный комментарий.
		
	\subsection{Хранение}
		После получения все комментарии будут сохраняться в отдельном каталоге comments, который будет создаваться в случае его отсутсвии, где каждый файл с коменатриями будет иметь название \textbf{video\_id.txt}, для возможности индефекации.		
		Для упрощения процесса хранения и доступа к данным, а также для обеспечения их совместимости с различными инструментами анализа, принято решение использовать обычный текстовый файл (.txt) для хранения комментариев. Каждый комментарий будет записан в отдельную строку файла, что упрощает структурирование данных и позволяет легко считывать их для анализа.
	Такой способ хранения данных был выбран по нескольким причинам:
	\begin{itemize}
		\item \textbf{Простота реализации}: Текстовые файлы легко создаются, читаются и редактируются с помощью множества программных средств и языков программирования.
		\item \textbf{Совместимость}: Практически все аналитические инструменты и библиотеки могут работать с текстовыми файлами, что облегчает интеграцию данных в различные этапы анализа.
		\item \textbf{Малое влияние на анализ}: Формат хранения не оказывает значительного влияния на сам процесс анализа данных. Основное внимание уделяется содержимому комментариев, а не способу их хранения.
	\end{itemize}

\section{Обработка текста}

	\subsection{Opis}
		Следующим процессом является обработка текста. Важно понимать, как выглядят комментарии после их получения и сохранения с YouTube. Реализация обработки будет выполнена с помощью библиотеки NLTK (Natural Language Toolkit) и встроенных функций Python. Ознакомиться с документацией NLTK можно здесь \cite{nltk}. Обработка текста разделена на несколько этапов:
		\begin{itemize}
			\item Обязательная обработка выполняющеся независимо от выбораных параметров пользователем.
			\item Примитивная обрботка удаление символ  и цифр
			\item Удаление stop-words
			\item Лематизация
			\item Aнализ
		\end{itemize}
		Все методы кроме обязательной обработки и анализа, являются необязательным, то есть будут выполняться только в случае если пользователь укажет что хочет использовать данные методы.
		
	\subsection{Обязательная обработка}
		Обязательными этапами обработки текста являются приведение слов к нижнему регистру и процесс токенизации, который устраняет необходимость удаления пробелов. С помощью функции \textbf{nltk.word tokenize} каждое слово будет разделено на токены. Эти этапы обязательны, так как они упрощают дальнейшую работу с текстом и гарантируют правильную работу многих последующих функций, которые могут быть чувствительны к регистру.
		
	\subsection{Примитивная обработка}
		Примитивная обработка подразумевает удаление неалфавитных символов или слов. Этот процесс не является обязательным, однако его использование помогает избавиться от различных языков разметки, которые YouTube использует для визуализации смайликов или инструментов изменения текста. 
		Пример таких комментариев:
		\begin{verbatim}
			"Great video! (here should be emoji"
			"Check this out: <a href='https://example.com'>link</a>"
		\end{verbatim}
		В данном проекте это реализовано с помощью встроенной функции Python \textbf{isalpha()}, которая проверяет символы или слова на наличие неалфавитных символов.
		\begin{lstlisting}[language=Python]
			def remove_non_alpha(tokens):
			return [word for word in tokens if word.isalpha()]
		\end{lstlisting}
		
	\subsection{Удаление stop-words}
		Этот этап реализует удаление слов, которые не несут значимой информации. Типичными примерами таких слов являются:
		\begin{verbatim}
			"the", "is", "in", "and"
		\end{verbatim}
		Удаление стоп-слов уменьшает размер текста и делает информативные слова более заметными. Для этого используется словарь стоп-слов из библиотеки NLTK, который в случае необходимости можно дополнить своими словами.
		\begin{lstlisting}[language=Python]
		from nltk.corpus import stopwords

		def remove_stop_words(tokens):
			stop_words = set(stopwords.words('english'))
			return [word for word in tokens if word not in stop_words]
		\end{lstlisting}

	\subsection{Лематизация}
		Последним необязательным процессом является лемматизация, которая приводит слова к их базовой форме. Это полезно в случаях, когда нам нужно сгруппировать разные формы одного слова (например, "running", "ran", "runs" к "run").

		\begin{lstlisting}[language=Python]
		from nltk.stem import WordNetLemmatizer

		def lemmatize_tokens(tokens):
			lemmatizer = WordNetLemmatizer()
		return [lemmatizer.lemmatize(word) for word in tokens]
		\end{lstlisting}

	
	\subsection{Анализ}
		В конце, независимо от выбранных пользователем этапов, все токены проверяются на непустые значения, чтобы избежать полностью пустых слов и строк. Далее все токены объединяются в одну строку, разделённую пробелами. На этом этапе результат обработки возвращается и анализируется функциями частотного анализа (\textbf{freq\_analysis}) и анализа длины (\textbf{length\_analysis}). Эти функции будут использоваться для построения графиков, которые помогут пользователю визуально оценить частоту встречаемых слов и длины комментариев для последующего принятия решений.
	
		\subsubsection{freq analysis}
			Эта функция использует обработанные строки, создавая единый массив со всеми словами из всех комментариев, и применяет класс \textbf{collections.Counter} в Python. \textbf{Counter} используется для подсчёта хэшируемых объектов, таких как строки или числа, и предоставляет удобные методы для работы с частотными данными. Таким образом, функция генерирует два массива: массив слов и соответствующий ему массив количества каждого слова в тексте. Эти массивы будут использоваться для построения столбчатой диаграммы (bar chart), показывающей частоту встречаемости слов.
				
			\begin{verbatim}
				from collections import Counter
				import matplotlib.pyplot as plt

				def freq_analysis(tokens):
				word_counts = Counter(tokens)
				words = list(word_counts.keys())
				counts = list(word_counts.values())
				plt.bar(words, counts)
				plt.xlabel('Words')
				plt.ylabel('Frequency')
				plt.title('Word Frequency Analysis')
				plt.show()
			\end{verbatim}
		
		\subsubsection{length analysis}
			\underline{возможно тут еще буду менять} \\
			Эта функция также использует обработанные строки комментариев, определяя длину каждого комментария. В конечном итоге генерируется массив длин, который будет использован для построения гистограммы (histogram), показывающей распределение длины комментариев.
	
	\subsection{Итог}
		Все этапы обработки текста, от обязательных до необязательных, играют важную роль в подготовке данных для дальнейшего анализа. Их правильное выполнение гарантирует структурированный и удобный вид для обработки. В визуализация графиков анализруещих содержания комментариев позволит пользователям лучше понимать структуру и содержание комментариев, а также принимать обоснованные решения на основе представленных данных.
	
	
\section{Wektoryzacja}
	
	\subsection{Opis}
		На данном этапе текст готов к следующему этапу обработки. Для последующего анализа необходимо преобразовать комментарии в векторную форму (векторизировать). Будт использовано два метода: TF-IDF и Word2Vec. Эти методы позволят представить текстовые данные в виде числовых векторов, что облегчит дальнейший анализ и обработку. Векторизация будет реализована с использованием библиотек \textbf{scikit-learn} для метода TF-IDF и \textbf{gensim} для метода Word2Vec.
	
	\subsection{TF-IDF}
 		TF-IDF (Term Frequency-Inverse Document Frequency) — это статистическая мера, используемая для оценки важности термина в документе относительно коллекции документов (корпуса). Она широко применяется в задачах обработки естественного языка и информационного поиска. В библиотеке Scikit-learn существует удобный инструмент для расчета TF-IDF: класс TfidfVectorizer.
		
		\subsubsection{Пояснение алгоритма}
			\begin{itemize}
				\item Term Frequency (TF)
				\item Inverse Document Frequency (IDF)
				\item TF-IDF
			\end{itemize}
			

		\subsubsection{Параметры }
			\begin{itemize}
				\item max df: Число или доля. Термины, которые встречаются в большем числе документов, чем указано, игнорируются. Полезно для удаления общеупотребительных слов.
				\item min df: Число или доля. Термины, которые встречаются в меньшем числе документов, чем указано, игнорируются. Полезно для удаления редких слов.
				\item ngram range: Тупл (min n, max n). Определяет диапазон для извлечения n-грамм.
				\item stop words: Список слов или строка, определяющая слова, которые следует игнорировать. 
			\end{itemize}
		
		
		\subsubsection{Пример использования}
			\begin{verbatim}
				from sklearn.feature_extraction.text import TfidfVectorizer
	
				def vectorize_tfidf(corpus):
					vectorizer = TfidfVectorizer()
					tfidf_matrix = vectorizer.fit_transform(corpus)
				return tfidf_matrix
			\end{verbatim}

			Пример до и после:
			\begin{itemize}
				\item До: ["This is a great video", "I love this content"]
				\item После: [[0.5, 0.5, 0, 0.5, 0, 0.5], [0, 0, 0.7, 0, 0.7, 0]]
			\end{itemize}
			
			Для работы с TF-IDF необходимо иметь корпус текстов, который можно представить в виде списка строк, где каждая строка — это отдельный документ.
			
 
	\subsection{Word2Vec}
 		Word2Vec — это алгоритм, разработанный для обучения векторных представлений слов (эмбеддингов), которые захватывают семантические отношения между словами. В библиотеке Gensim предоставляется удобный интерфейс для работы с Word2Vec.
		
		\subsubsection{Важные параметры}	
			\begin{itemize}
				\item sentences: Корпус текстов, представленный в виде списка списков слов.
				\item vector size: Размерность векторного представления слов.
				\item window: Максимальное расстояние между текущим и предсказанным словом в предложении.
				\item min count: Минимальная частота слова в корпусе для его учета.
				\item workers: Количество потоков для параллельного обучения.
			\end{itemize}
		
			Более глабальные настройки
			\begin{itemize}
				\item SG (Skip-Gram) и CBOW (Continuous Bag of Words): В Word2Vec есть два основных архитектурных подхода — Skip-Gram и CBOW. Параметр sg в Gensim позволяет выбрать между ними:
					\begin{itemize}
						\item sg=0 использует CBOW (по умолчанию).
						\item sg=1 использует Skip-Gram.
					\end{itemize}
				\item Negative Sampling: В Word2Vec используется негативное сэмплирование для повышения эффективности обучения. Параметр negative задает количество "негативных" примеров для каждого положительного примера.
				\item Learning Rate: Параметр alpha задает начальную скорость обучения, а min alpha — минимальную скорость обучения, которая уменьшается до этого значения по мере обучения.
			\end{itemize}
		
		\subsubsection{Описание алгоритма}
			\begin{enumerate}
				\item Инициализация весов:
Веса нейронной сети инициализируются случайными значениями. Это веса между входным слоем и скрытым слоем, а также между скрытым слоем и выходным слоем.
				\item Прямой проход (Forward Pass):
Для каждого слова в предложении создается входной вектор. В случае Skip-Gram этот входной вектор используется для предсказания контекстных слов. В случае CBOW контекстные слова используются для предсказания текущего слова.
				\begin{itemize}
					\item Skip-Gram:
					\begin{itemize}
						\item Входное слово кодируется в виде одноразового вектора.
						\item Одноразовый вектор умножается на веса между входным и скрытым слоями для получения скрытого представления.
						\item Скрытое представление умножается на веса между скрытым и выходным слоями для предсказания контекстных слов.
					\end{itemize}
					\item CBOW:
					\begin{itemize}
						\item Контекстные слова кодируются в виде одноразовых векторов.
						\item Одноразовые векторы суммируются и умножаются на веса между входным и скрытым слоями для получения скрытого представления.
						\item Скрытое представление умножается на веса между скрытым и выходным слоями для предсказания текущего слова.
					\end{itemize}								
				\end{itemize}
				\item Обратное распространение (Backpropagation):
Ошибка предсказания рассчитывается путем сравнения предсказанного слова с фактическим. Затем эта ошибка распространяется обратно через сеть для обновления весов. Используется метод стохастического градиентного спуска (SGD) и, в некоторых случаях, негативное сэмплирование для ускорения обучения.
				\begin{itemize}
					\item Обновление весов:
Веса обновляются в направлении, противоположном градиенту ошибки. Это помогает минимизировать ошибку предсказания.
				\end{itemize}
				\item Повторение процесса:
Этот процесс повторяется для всех слов в предложении и для всех предложений в корпусе. Модель проходит через корпус несколько раз, как определено параметром epochs.
			\end{enumerate}
	
	
 		\subsubsection{Пример использования}
 			\begin{verbatim}
				from gensim.models import Word2Vec
	
				def vectorize_word2vec(tokens):
				model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4)
				word_vectors = model.wv
				return word_vectors
			\end{verbatim}

			Пример до и после:
			\begin{itemize}
				\item До: ["This is a great video", "I love this content"]
				\item После: Векторы слов: { "this": [...], "is": [...], "a": [...], "great": [...], "video": [...], "I": [...], "love": [...], "content": [...] }
			\end{itemize}
 	
 	\subsection{Ocena}
 		После векторизации текста с использованием методов TF-IDF и Word2Vec важно провести оценку их эффективности и понять, насколько хорошо они справляются с задачей представления текстовых данных. В данной секции будут рассмотрены критерии оценки и результаты применения каждого из методов на наших данных.
 	
 		\subsubsection{Критерии}
 			Для оценки работы методов TF-IDF и Word2Vec используются следующие критерии:

			\begin{itemize}
				\item \textbf{Семантическая значимость:} Насколько хорошо метод улавливает семантические связи 	между словами и контекст их использования.
				\item \textbf{Скорость вычислений:} Время, необходимое для векторизации текстов.
				\item \textbf{Простота интерпретации:} Насколько легко интерпретировать результаты векторизации и использовать их в дальнейших анализах.
				\item \textbf{Применимость к задачам машинного обучения:} Насколько хорошо векторы подходят для задач классификации, кластеризации и других методов машинного обучения.
			\end{itemize}
		
		\subsubsection{Оценка TF-IDF}
			\underline{Даже сам еще не делал}
		
		\subsubsection{Оценка Word2Vec}
			Какая то имееться
		
 	
 	\subsection{Итог}
		незнаю
	
\section{Кластеризация}

	\subsection{Opis}
		Следующим этапом обработки данных является кластеризация, которая использует векторезированные комментарии и разбивает их на кластеры. Реализовано это будет с помощь дву методов K-means и DBSCAN используя библиотеку sklearn.
	
	\subsection{K-means}
 		Алгоритм K-means  разбивает набор данных на непересекающиеся кластеры, каждый из которых описывается средним значением (центроидом) образцов в кластере. K-means стремится выбрать центроиды, минимизируя инерцию, или критерий суммы квадратов внутри кластеров.
	
		\subsubsection{Описание алгоритма}
			Алгоритм K-means можно понять через три основных шага. Первый шаг заключается в выборе начальных центроидов. Обычно используется простой метод выбора случайных образцов из набора данных. После инициализации K-means выполняется последовательное выполнение двух других шагов. Первый шаг заключается в присвоении каждого образца ближайшему центроиду. Второй шаг создает новые центроиды, вычисляя среднее значение всех образцов, принадлежащих каждому предыдущему центроиду. Разница между старыми и новыми центроидами вычисляется, и алгоритм повторяет эти два последних шага до тех пор, пока эта разница не станет менее определенного порога. Другими словами, алгоритм повторяется, пока центроиды не перестанут существенно перемещаться.

			Как и многие алгоритмы, K-means подвержен проблеме локальных минимумов, что сильно зависит от начальной инициализации центроидов. Обычно алгоритм запускается несколько раз с различными инициализациями центроидов.
			
		\subsubsection{Параметры}
			Для настройки алгоритма K-means используются следующие параметры:
			\begin{itemize}
				\item n clusters: Количество кластеров (K), на которые нужно разделить данные.
				\item init: Метод инициализации центроидов.
				\item max iter: Максимальное количество итераций для одного запуска K-means.
				\item tol: Порог для остановки алгоритма. Если разница между старыми и новыми центроидами меньше этого значения, алгоритм останавливается.
				\item n init: Количество запусков алгоритма с различными инициализациями. Результат с наименьшей инерцией будет выбран как окончательный.
				\item random state: Начальное значение генератора случайных чисел для воспроизводимости результатов.
				\item algorithm: Алгоритм для вычисления K-means. Возможные значения включают 'auto', 'full', и 'elkan'.
			\end{itemize}
			
		\subsubsection{Пример использования}
			\begin{verbatim}
				kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10, random_state=0)

				# Обучение модели
				kmeans.fit(X)

				# Предсказание кластеров для данных
				y_kmeans = kmeans.predict(X)
			\end{verbatim}
			
			
			Пример преобразования данных:
				были точки, а теперь число которое показывет в каком кластере.
	  
	\subsection{DBSCAN}
		Алгоритм DBSCAN (Density-Based Spatial Clustering of Applications with Noise) рассматривает кластеры как области высокой плотности, отделенные от областей низкой плотности. Это позволяет DBSCAN обнаруживать кластеры любой формы, в отличие от K-means, который предполагает, что кластеры имеют выпуклую форму. Основной компонент DBSCAN - это понятие ядерных образцов, которые представляют собой образцы в областях высокой плотности.
		
		\subsubsection{Параметры}
			Парметры алгоритам DBSCAN:
			\begin{itemize}
				\item eps: Максимальное расстояние между двумя образцами, при котором один считается соседом другого. Это параметр радиуса для определения плотности области.
				\item min samples: Минимальное число образцов (включая сам образец), необходимое для того, чтобы область считалась плотной, т.е. чтобы образец стал ядерным.
				\item metric: Метрика для вычисления расстояний между точками (по умолчанию 'euclidean'). Это может быть любая метрика, поддерживаемая функцией scipy.spatial.distance.pdist.
				\item algorithm: Алгоритм, используемый для вычисления ближайших соседей ('auto', 'ball tree', 'kd tree', 'brute').
				\item leaf size: Размер листа, передаваемый алгоритмам 'ball tree', 'kd tree'. Это может повлиять на скорость построения и запроса ближайших соседей.
				\item p: Степень Минковского расстояния, когда используется метрика 'minkowski' (по умолчанию p=2, что соответствует евклидову расстоянию).	
			\end{itemize}
			
			
		\subsubsection{Описание алгоритма}
			Алгоритм DBSCAN реализуется следующим образом:
			\begin{enumerate}
				\item Определяются ядерные образцы: Образцы, для которых количество соседей в пределах eps не меньше min samples.
				\item Строятся кластеры: Для каждого ядерного образца строится кластер путем нахождения всех его соседей, которые также являются ядерными образцами, и их собственных соседей и так далее. Каждый кластер также включает не-ядерные образцы, которые являются соседями ядерных образцов в кластере.
				\item Выделяются выбросы: Все образцы, которые не являются ядерными и не находятся на достаточном расстоянии от ядерных образцов, считаются выбросами.
			\end{enumerate}
			Алгоритм DBSCAN детерминирован и всегда генерирует одинаковые кластеры при одинаковых входных данных в одном и том же порядке. Однако результаты могут отличаться при предоставлении данных в другом порядке. Это происходит из-за того, что порядок обработки данных влияет на то, каким образом назначаются образцы кластерам.
			
		\subsubsection{Пример использования}
			\begin{verbatim}
				X = np.array([[0, 1], [1, 1], [2, 1], [3, 1], [4, 1],
             			 [0, 0], [1, 0], [2, 0], [3, 0], [4, 0]])

					# Создание модели t-SNE
					tsne = TSNE(n_components=2, perplexity=5, n_iter=300, random_state=0)

					# Преобразование данных
					X_embedded = tsne.fit_transform(X)
			\end{verbatim}
			
			
			Пример преобразования данных:
				были точки, а теперь число которое показывет в каком кластере.
		
	\subsection{Выбор параметров}
			
	
	
\section{Визуализация}
	\subsection{Opis}
		Заключителый процесс анализа это визуализация данных. Итоговый результат представляет из себя \textbf{2D Plot}, где каждая точка будет представлять некоторые текстовые данные (комментарии) с опрелённым цветом, соотвутсвующим цвету кластера, которому она принадлежит, или же цвету отсутсвия кластера как в случае DBSCAN. Так же на графике будут визуализироваться центра каждого кластера в виде красного крестика. Для удобства навигации и проверки результатов  при наведении на точку(коментарий) или крустик(центр кластера) будет высчевиваться информация с содержанием элемента, которая репрезентуется с помощью библиотеки \textbf{mplcursor}. В случае точки это содержание обработоного комментария (? или лучше необработаных ?). В случае крестика репрезентуется центр кластера, которая с помощью метода обратного векторизации будет пытаться представить данную точку как слово.
	
	\subsection{Cнижение размерности}
		Для возможности визуализации векторизированых данных в качестве 2D точек необходимо воспользоваться методом снижения размерности (T-SNE). Данный метод является нелейным методом выделения признаков. Изначально трудно определить за что отвечают признаки, то выбрать какие-то наиболее подходящие признаки невозможно. Нелейный метод поможет нам сохранить неленейные связи. \textbf{Тут тоже желательно пример различий методов снижения размерности}.
		
	\subsection{T-SNE}
	
		\subsubsection{Параметры}
			\begin{itemize}
				\item n\_components: Количество измерений, в которое нужно проецировать данные (обычно 2 или 3 для визуализации).
				\item perplexity: Параметр, связанный с количеством ближайших соседей. Он влияет на баланс между локальной и глобальной структурой данных.
				\item early exaggeration: Параметр, который контролирует расстояния между точками в начальной фазе оптимизации.
				\item learning rate: Скорость обучения для градиентного спуска.
				\item n iter: Количество итераций для оптимизации.
				\item metric: Метрика для вычисления расстояний между точками (по умолчанию 'euclidean').
				\item init: Метод инициализации ('random' или 'pca').			
				\item random state: Начальное значение генератора случайных чисел для воспроизводимости результатов.
			\end{itemize}				
		
		\subsubsection{Описание алгоритма}
			Алгоритм t-SNE выполняется следующим образом:
			\begin{enumerate}
				\item Преобразование расстояний в вероятности: Для каждой точки рассчитываются вероятности Пирсона, которые определяют вероятность того, что соседняя точка будет выбрана в качестве соседа.
				\item Определение целевых вероятностей: На низкоразмерной проекции рассчитываются вероятности Q, аналогичные вероятностям Пирсона, но в другой размерности.
				\item  Минимизация расхождения (Kullback-Leibler divergence): Градиентный спуск используется для минимизации расхождения между распределениями P и Q, что приводит к сохранению близости точек.
			\end{enumerate}
			
			
		\subsubsection{Пример использования}
			\begin{verbatim}
				X = np.array([[1, 2], [2, 2], [2, 3],
              					[8, 7], [8, 8], [25, 80]])

				# Создание модели DBSCAN
				dbscan = DBSCAN(eps=3, min_samples=2)

				# Обучение модели
				dbscan.fit(X)

				# Предсказание кластеров для данных
				labels = dbscan.labels_
			\end{verbatim}
			
			
			Пример преобразования данных:
				были точки, а теперь число которое показывет в каком кластере.
	
	






