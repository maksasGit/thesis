\chapter{Rozdzial 2}
	
		
\section{Aplikacja}

	\subsection{Opis aplikacji}
		Ten projekt ma na celu stworzenie narzędzia do analizy danych tekstowych, obejmującego wstępne przetwarzanie tekstu, wektoryzację i klasteryzację. Głównym celem aplikacji jest umożliwienie użytkownikowi elastycznego dostosowania i wykorzystania różnych metod przetwarzania danych do rozwiązywania konkretnych zadań.
		
		
	\subsection{Technologie}
		Do realizacji projektu wybrano język programowania Python. Posiada on szereg zalet: \cite{Python}
		\begin{itemize}
			\item Bogaty zestaw bibliotek do implementacji różnych metod, takich jak wstępne przetwarzanie tekstu, wektoryzacja i klasteryzacja.
			\item Wysoka efektywność w analizie danych dzięki bibliotekom takim jak NumPy, scikit-learn.
			\item Społeczność i dokumentacja, które ułatwiają rozwój i utrzymanie projektów.
		\end{itemize}
		
		
	\subsection{GUI}
		Główną ideą aplikacji jest stworzenie wygodnego narzędzia do analizy danych tekstowych, w którym użytkownik może samodzielnie wybierać i dostosowywać metody przetwarzania do konkretnych zadań za pomocą interfejsu graficznego. Do opracowania interfejsu graficznego wykorzystano biblioteki PyQt i Matplotlib:
		\begin{itemize}
			\item PyQt: Umożliwia tworzenie intuicyjnych interfejsów użytkownika.
			\item Matplotlib: Wykorzystywana do wizualizacji danych, co pozwala użytkownikowi łatwo interpretować wyniki analizy.
		\end{itemize}
		Interfejs graficzny umożliwia użytkownikowi ładowanie danych, konfigurowanie parametrów przetwarzania tekstu, wektoryzacji i klasteryzacji oraz wizualizację wyników.
		
		
		
	\subsection{OOP}
		Głównym komponentem mojej aplikacji jest klasa \textbf{Workspace}, która stanowi kontener przechowujący parametry dla trzech głównych procesów:
		\begin{itemize}
			\item Przetwarzanie tekstu: Obejmuje metody wstępnego przetwarzania, takie jak tokenizacja, usuwanie stop-słów i lematyzacja.
			\item Wektoryzacja: Przekształca dane tekstowe w wektory liczbowe przy użyciu metod takich jak TF-IDF lub Word2Vec.
			\item Klasteryzacja: Implementuje algorytmy, takie jak K-means lub DBSCAN, do grupowania danych tekstowych.
		\end{itemize}
		Klasa \textbf{Workspace} zapewnia strukturalne przechowywanie i zarządzanie parametrami każdego etapu, co upraszcza proces konfiguracji i wykonania analizy.
		
	
\section{Dane tekstowe}

	\subsection{Opis}
		Aby zrealizować ten projekt, potrzebne będą dane tekstowe, które w tym przypadku będą komentarzami z YouTube. Do pozyskiwania informacji o filmach i powiązanych komentarzach będziemy używać YouTube Data API v3. API to jest darmowe i łatwe w użyciu. Aby rozpocząć pracę, należy zarejestrować się i utworzyć projekt w Google Cloud, a następnie aktywować YouTube Data API. Po tym wygenerowany zostanie klucz API, który będziemy używać do uwierzytelniania.
		
	\subsection{Zapytania API}
		W projekcie wykorzystane będą dwa zapytania HTTP:
		\begin{itemize}
			\item "GET https://www.googleapis.com/youtube/v3/commentThreads"
			\item "GET https://www.googleapis.com/youtube/v3/videos"
		\end{itemize}
	 	Pierwsze zapytanie służy do pobierania komentarzy do konkretnego filmu, a drugie do uzyskiwania informacji o filmie. Szczegółowe informacje można znaleźć w dokumentacji \cite{YouTubeAPI}. Etapy pobierania komentarzy:
	 	\begin{enumerate}
	 		\item Użytkownik wprowadza w interfejsie graficznym (GUI) link do interesującego go filmu.
	 		\item Program wyodrębnia \textbf{video\_id} z podanego linku.
	 		\item Za pomocą \textbf{video\_id} wykonywane jest zapytanie GET https://www.googleapis.com/youtube/v3/videos, aby uzyskać podstawowe informacje o filmie. To pozwala użytkownikowi upewnić się, że wybrany film odpowiada jego oczekiwaniom.
	 		\item Po potwierdzeniu przez użytkownika, że film jest prawidłowy, wykonywane jest zapytanie GET https://www.googleapis.com/youtube/v3/commentThreads, aby pobrać komentarze do tego filmu.
	 		\item Uzyskane komentarze są zapisywane w pliku tekstowym, gdzie każdy komentarz jest zapisany w oddzielnej linii. Nazwa pliku odpowiada unikalnemu identyfikatorowi filmu (\textbf{video\_id}).
	 	\end{enumerate}
	
	\subsection{Przetwarzanie uzyskanych danych}
		Wyniki naszych zapytań będą dostarczane w formacie JSON. Szczegółowe informacje o strukturze odpowiedzi JSON można znaleźć w dokumentacji \cite{YouTubeAPI}. Do uzyskiwania informacji o filmie będą używane tylko następujące pola:
		\begin{itemize}
			\item title: tytuł filmu
			\item channelTitle: nazwa autora kanału
			\item thumbnails: URL miniaturki
		\end{itemize}				
		 Przy pobieraniu komentarzy potrzebne są tylko następujące pola:
		 \begin{itemize}
		 	\item textDisplay: treść komentarza
		 	\item replies: kontener z odpowiedziami na komentarze
		 \end{itemize}
 		 Pole \textbf{textDisplay} zawiera tekst komentarza, a pole \textbf{replies} jest kontenerem na odpowiedzi na komentarze. Każda odpowiedź w \textbf{replies.comments} również ma pole \textbf{textDisplay}, które możemy osobno przetwarzać. W końcowym wyniku otrzymamy listę wszystkich komentarzy, gdzie każdy element reprezentuje oddzielny komentarz.
		
	\subsection{Przechowywanie}
		Po uzyskaniu wszystkie komentarze będą przechowywane w osobnym katalogu \textbf{comments}, który zostanie utworzony w przypadku jego braku. Każdy plik z komentarzami będzie miał nazwę \textbf{video\_id.txt} dla łatwej identyfikacji.
		Dla uproszczenia procesu przechowywania i dostępu do danych oraz zapewnienia ich kompatybilności z różnymi narzędziami analitycznymi, zdecydowano się na użycie zwykłego pliku tekstowego (.txt) do przechowywania komentarzy. Każdy komentarz będzie zapisany w oddzielnej linii pliku, co upraszcza strukturę danych i umożliwia łatwe ich odczytanie do analizy.
		Taki sposób przechowywania danych został wybrany z kilku powodów:
	\begin{itemize}
		\item \textbf{Prostota implementacji}: Pliki tekstowe łatwo tworzyć, czytać i edytować za pomocą wielu narzędzi programistycznych i języków programowania.
		\item \textbf{Kompatybilność}: Praktycznie wszystkie narzędzia analityczne i biblioteki mogą pracować z plikami tekstowymi, co ułatwia integrację danych w różnych etapach analizy.
		\item \textbf{Mały wpływ na analizę}: Format przechowywania nie ma znaczącego wpływu na sam proces analizy danych. Główne znaczenie ma zawartość komentarzy, a nie sposób ich przechowywania.
	\end{itemize}

\section{Przetwarzania tekstu}

	\subsection{Defenicja}
		Następnym procesem jest przetwarzanie tekstu. Ważne jest, aby zrozumieć, jak wyglądają komentarze po ich pobraniu i zapisaniu z YouTube. Implementacja przetwarzania zostanie wykonana za pomocą biblioteki NLTK (Natural Language Toolkit) oraz wbudowanych funkcji Pythona. Dokumentację NLTK można znaleźć tutaj \cite{nltk}. Przetwarzanie tekstu jest podzielone na kilka etapów:
		\begin{itemize}
			\item Przetwarzanie obowiązkowe, które jest wykonywane niezależnie od wybranych parametrów przez użytkownika.
			\item Przetwarzanie prymitywne, czyli usuwanie symboli i cyfr.
			\item Usuwanie stop-words.
			\item Lematyzacja.
			\item Analiza.
		\end{itemize}
		Wszystkie metody, oprócz przetwarzania obowiązkowego i analizy, są opcjonalne, co oznacza, że będą wykonywane tylko wtedy, gdy użytkownik wskaże, że chce używać tych metod.
		
	\subsection{Przetwarzanie obowiązkowe}
		Obowiązkowe etapy przetwarzania tekstu to zmiana wszystkich słów na małe litery oraz proces tokenizacji, który eliminuje potrzebę usuwania spacji. Za pomocą funkcji \textbf{nltk.word_tokenize} każde słowo zostanie podzielone na tokeny. Te etapy są obowiązkowe, ponieważ upraszczają dalszą pracę z tekstem i gwarantują poprawne działanie wielu późniejszych funkcji, które mogą być czułe na wielkość liter.
	
		
	\subsection{Przetwarzanie prymitywne}
		Przetwarzanie prymitywne polega na usuwaniu niealfabetycznych symboli lub słów. Ten proces nie jest obowiązkowy, jednak jego zastosowanie pomaga pozbyć się różnych języków znaczników, które YouTube używa do wizualizacji emotikonów lub narzędzi do zmiany tekstu.

		Przykłady takich komentarzy:

		\begin{verbatim}
			"Great video! (tu musi byc emoji, ale nie wiem jak tu wkleic)"
			"Check this out: <a href='https://example.com'>link</a>"
		\end{verbatim}
		W tym projekcie jest to realizowane za pomocą wbudowanej funkcji Pythona \textbf{isalpha()}, która sprawdza, czy symbole lub słowa zawierają niealfabetyczne znaki.
		\begin{lstlisting}[language=Python]
			def remove_non_alpha(tokens):
			return [word for word in tokens if word.isalpha()]
		\end{lstlisting}
		
	\subsection{Usuwanie stop-words}
		Ten etap polega na usuwaniu słów, które nie niosą znaczącej informacji. Typowymi przykładami takich słów są:
		\begin{verbatim}
			"the", "is", "in", "and"
		\end{verbatim}
		Usuwanie stop-words zmniejsza rozmiar tekstu i sprawia, że znaczące słowa są bardziej widoczne. Do tego celu używa się słownika stop-words z biblioteki NLTK, który w razie potrzeby można uzupełnić własnymi słowami.
		\begin{lstlisting}[language=Python]
		from nltk.corpus import stopwords

		def remove_stop_words(tokens):
			stop_words = set(stopwords.words('english'))
			return [word for word in tokens if word not in stop_words]
		\end{lstlisting}

	\subsection{Lematyzacja}
		Ostatnim opcjonalnym procesem jest lematyzacja, która sprowadza słowa do ich podstawowej formy. Jest to przydatne w sytuacjach, gdy trzeba zgrupować różne formy jednego słowa (np. "running", "ran", "runs" do "run").

		\begin{lstlisting}[language=Python]
		from nltk.stem import WordNetLemmatizer

		def lemmatize_tokens(tokens):
			lemmatizer = WordNetLemmatizer()
		return [lemmatizer.lemmatize(word) for word in tokens]
		\end{lstlisting}

	
	\subsection{Analiza}
		Na końcu, niezależnie od wybranych przez użytkownika etapów, wszystkie tokeny są sprawdzane pod kątem niepustych wartości, aby uniknąć całkowicie pustych słów i wierszy. Następnie wszystkie tokeny są łączone w jeden ciąg znaków, oddzielony spacjami. Na tym etapie wynik przetwarzania jest zwracany i analizowany za pomocą funkcji analizy częstotliwości (\textbf{freq\_analysis}) i analizy długości (\textbf{length\_analysis}). Te funkcje będą używane do tworzenia wykresów, które pomogą użytkownikowi wizualnie ocenić częstotliwość występowania słów i długość komentarzy w celu podejmowania dalszych decyzji.
	
		\subsubsection{Analiza częstotliwości (freq analysis)}
			Ta funkcja używa przetworzonych ciągów znaków, tworząc pojedynczą tablicę ze wszystkimi słowami ze wszystkich komentarzy, i stosuje klasę \textbf{collections.Counter} w Pythonie. \textbf{Counter} jest używany do zliczania hashowalnych obiektów, takich jak ciągi znaków lub liczby, i dostarcza wygodne metody do pracy z danymi częstotliwości. W ten sposób funkcja generuje dwie tablice: tablicę słów i odpowiadającą jej tablicę liczby wystąpień każdego słowa w tekście. Te tablice będą używane do tworzenia wykresu słupkowego (bar chart), pokazującego częstotliwość występowania słów.
			
			Przykład implementacji algorytmu w projekcie:
			\begin{lstlisting}[language=Python]
				from collections import Counter
				import matplotlib.pyplot as plt

				def freq_analysis(tokens):
					data = ' '.join(self.nlp_data)
					words = nltk.word_tokenize(data)
					# words = [word.lower() for word in words]
					# words = [word for word in words if word.isalnum()]
					word_freq = Counter(words)
					words_counts_sorted = word_freq.most_common()
					self.words_sorted = [word_count[0] for word_count in words_counts_sorted]
					self.counts_sorted = [word_count[1] for word_count in words_counts_sorted]
			\end{lstlisting}
		
		\subsubsection{Analiza długości (length analysis)}
			Ta funkcja również używa przetworzonych ciągów znaków komentarzy, określając liczbę słów w każdym komentarzu. Ostatecznie generowana jest tablica z liczbą słów, która będzie używana do tworzenia histogramu (histogram), pokazującego rozkład długości komentarzy.
			
			Przykład implementacji algorytmu w projekcie:
			\begin{lstlisting}[language=Python]
				from collections import Counter
				import matplotlib.pyplot as plt
				
				def freq_analysis(tokens):
					word_counts = [len(nltk.word_tokenize(el)) for el in self.nlp_data]
					self.max_size = max(word_counts) + 1
					self.lengths = [0] * (self.max_size + 1)
					for count in word_counts:
					self.lengths[count] += 1
					endTime = time.time()
			\end{lstlisting}
	
	\subsection{Podsumowanie}
		Wszystkie etapy przetwarzania tekstu, od obowiązkowych do opcjonalnych, odgrywają ważną rolę w przygotowaniu danych do dalszej analizy. Ich poprawne wykonanie gwarantuje strukturalizowany i wygodny do przetwarzania wygląd danych. Wizualizacja wykresów analizujących zawartość komentarzy pozwoli użytkownikom lepiej zrozumieć strukturę i treść komentarzy, a także podejmować uzasadnione decyzje na podstawie przedstawionych danych.
	
	
\section{Wektoryzacja}
	
	\subsection{Opis}
		На данном этапе текст готов к следующему этапу обработки. Для последующего анализа необходимо преобразовать комментарии в векторную форму (векторизировать). Будт использовано два метода: TF-IDF и Word2Vec. Эти методы позволят представить текстовые данные в виде числовых векторов, что облегчит дальнейший анализ и обработку. Векторизация будет реализована с использованием библиотек \textbf{scikit-learn} для метода TF-IDF и \textbf{gensim} для метода Word2Vec.
	
	\subsection{TF-IDF}
 		TF-IDF (Term Frequency-Inverse Document Frequency) — это статистическая мера, используемая для оценки важности термина в документе относительно коллекции документов (корпуса). Она широко применяется в задачах обработки естественного языка и информационного поиска. В библиотеке Scikit-learn существует удобный инструмент для расчета TF-IDF: класс TfidfVectorizer.
		
		
		\subsubsection{Параметры }
			\begin{itemize}
				\item max\_df: Число или доля. Термины, которые встречаются в большем числе документов, чем указано, игнорируются. Полезно для удаления общеупотребительных слов.
				\item min\_df: Число или доля. Термины, которые встречаются в меньшем числе документов, чем указано, игнорируются. Полезно для удаления редких слов.
				\item ngram range: Тупл (min\_n, max\_n). Определяет диапазон для извлечения n-грамм.
				\item stop\_words: Список слов или строка, определяющая слова, которые следует игнорировать. 
			\end{itemize}
		
		\subsubsection{Пояснение алгоритма}
			\begin{itemize}
				\item Term Frequency (TF)
				\item Inverse Document Frequency (IDF)
				\item TF-IDF
			\end{itemize}
			
				
		\subsubsection{Пример использования}
			\begin{verbatim}
				from sklearn.feature_extraction.text import TfidfVectorizer
	
				def vectorize_tfidf(corpus):
					vectorizer = TfidfVectorizer()
					tfidf_matrix = vectorizer.fit_transform(corpus)
				return tfidf_matrix
			\end{verbatim}

			Пример преобразования текстовых данных:
			\begin{itemize}
				\item До: ["This is a great video", "I love this content"]
				\item После: [[0.5, 0.5, 0, 0.5, 0, 0.5], [0, 0, 0.7, 0, 0.7, 0]]
			\end{itemize}
			
			Для работы с TF-IDF необходимо иметь корпус текстов, который можно представить в виде списка строк, где каждая строка — это отдельный документ.
			
 
	\subsection{Word2Vec}
 		Word2Vec — это алгоритм, разработанный для обучения векторных представлений слов (эмбеддингов), которые захватывают семантические отношения между словами. В библиотеке Gensim предоставляется удобный интерфейс для работы с Word2Vec.
		
		\subsubsection{Важные параметры}	
			\begin{itemize}
				\item sentences: Корпус текстов, представленный в виде списка списков слов.
				\item vector\_size: Размерность векторного представления слов.
				\item window: Максимальное расстояние между текущим и предсказанным словом в предложении.
				\item min\_count: Минимальная частота слова в корпусе для его учета.
				\item workers: Количество потоков для параллельного обучения.
			\end{itemize}
		
			Более глабальные настройки
			\begin{itemize}
				\item SG (Skip-Gram) и CBOW (Continuous Bag of Words): В Word2Vec есть два основных архитектурных подхода — Skip-Gram и CBOW. Параметр sg в Gensim позволяет выбрать между ними:
					\begin{itemize}
						\item sg=0 использует CBOW (по умолчанию).
						\item sg=1 использует Skip-Gram.
					\end{itemize}
				\item Negative Sampling: В Word2Vec используется негативное сэмплирование для повышения эффективности обучения. Параметр negative задает количество "негативных" примеров для каждого положительного примера.
				\item Learning Rate: Параметр alpha задает начальную скорость обучения, а min\_alpha — минимальную скорость обучения, которая уменьшается до этого значения по мере обучения.
			\end{itemize}
		
		\subsubsection{Описание алгоритма}
			\begin{enumerate}
				\item Инициализация весов: Веса нейронной сети инициализируются случайными значениями. Это веса между входным слоем и скрытым слоем, а также между скрытым слоем и выходным слоем.
				\item Прямой проход (Forward Pass): Для каждого слова в предложении создается входной вектор. В случае Skip-Gram этот входной вектор используется для предсказания контекстных слов. В случае CBOW контекстные слова используются для предсказания текущего слова.
				\begin{itemize}
					\item Skip-Gram:
					\begin{itemize}
						\item Входное слово кодируется в виде одноразового вектора.
						\item Одноразовый вектор умножается на веса между входным и скрытым слоями для получения скрытого представления.
						\item Скрытое представление умножается на веса между скрытым и выходным слоями для предсказания контекстных слов.
					\end{itemize}
					\item CBOW:
					\begin{itemize}
						\item Контекстные слова кодируются в виде одноразовых векторов.
						\item Одноразовые векторы суммируются и умножаются на веса между входным и скрытым слоями для получения скрытого представления.
						\item Скрытое представление умножается на веса между скрытым и выходным слоями для предсказания текущего слова.
					\end{itemize}								
				\end{itemize}
				\item Обратное распространение (Backpropagation): Ошибка предсказания рассчитывается путем сравнения предсказанного слова с фактическим. Затем эта ошибка распространяется обратно через сеть для обновления весов. Используется метод стохастического градиентного спуска (SGD) и, в некоторых случаях, негативное сэмплирование для ускорения обучения.
				\begin{itemize}
					\item Обновление весов: Веса обновляются в направлении, противоположном градиенту ошибки. Это помогает минимизировать ошибку предсказания.
				\end{itemize}
				\item Повторение процесса: Этот процесс повторяется для всех слов в предложении и для всех предложений в корпусе. Модель проходит через корпус несколько раз, как определено параметром epochs.
			\end{enumerate}
	
	
 		\subsubsection{Пример использования}
 			\begin{verbatim}
				from gensim.models import Word2Vec
	
				def vectorize_word2vec(tokens):
				model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4)
				word_vectors = model.wv
				return word_vectors
			\end{verbatim}

			Пример преобразования текстовых данных:
			\begin{itemize}
				\item До: ["This is a great video", "I love this content"]
				\item После: Векторы слов: { "this": [...], "is": [...], "a": [...], "great": [...], "video": [...], "I": [...], "love": [...], "content": [...] }
			\end{itemize}
 	
 	\subsection{Ocena}
 		После векторизации текста с использованием методов TF-IDF и Word2Vec важно провести оценку их эффективности и понять, насколько хорошо они справляются с задачей представления текстовых данных. В данной секции будут рассмотрены критерии оценки и результаты применения каждого из методов на наших данных.
 	
 		\subsubsection{Критерии}
 			Для оценки работы методов TF-IDF и Word2Vec используются следующие критерии:

			\begin{itemize}
				\item \textbf{Семантическая значимость:} Насколько хорошо метод улавливает семантические связи 	между словами и контекст их использования.
				\item \textbf{Скорость вычислений:} Время, необходимое для векторизации текстов.
				\item \textbf{Простота интерпретации:} Насколько легко интерпретировать результаты векторизации и использовать их в дальнейших анализах.
				\item \textbf{Применимость к задачам машинного обучения:} Насколько хорошо векторы подходят для задач классификации, кластеризации и других методов машинного обучения.
			\end{itemize}
		
		\subsubsection{Оценка TF-IDF}
			
		
		\subsubsection{Оценка Word2Vec}

		
	
\section{Кластеризация}

	\subsection{Opis}
		Следующим этапом обработки данных является кластеризация, которая использует векторезированные комментарии и разбивает их на кластеры. Реализовано это будет с помощь дву методов K-means и DBSCAN используя библиотеку sklearn.
	
	\subsection{K-means}
 		Алгоритм K-means  разбивает набор данных на непересекающиеся кластеры, каждый из которых описывается средним значением (центроидом) образцов в кластере. K-means стремится выбрать центроиды, минимизируя инерцию, или критерий суммы квадратов внутри кластеров.
	
		\subsubsection{Описание алгоритма}
			Алгоритм K-means можно понять через три основных шага. Первый шаг заключается в выборе начальных центроидов. Обычно используется простой метод выбора случайных образцов из набора данных. После инициализации K-means выполняется последовательное выполнение двух других шагов. Первый шаг заключается в присвоении каждого образца ближайшему центроиду. Второй шаг создает новые центроиды, вычисляя среднее значение всех образцов, принадлежащих каждому предыдущему центроиду. Разница между старыми и новыми центроидами вычисляется, и алгоритм повторяет эти два последних шага до тех пор, пока эта разница не станет менее определенного порога. Другими словами, алгоритм повторяется, пока центроиды не перестанут существенно перемещаться.

			Как и многие алгоритмы, K-means подвержен проблеме локальных минимумов, что сильно зависит от начальной инициализации центроидов. Обычно алгоритм запускается несколько раз с различными инициализациями центроидов.
			
		\subsubsection{Параметры}
			Для настройки алгоритма K-means используются следующие параметры:
			\begin{itemize}
				\item n\_clusters: Количество кластеров (K), на которые нужно разделить данные.
				\item init: Метод инициализации центроидов.
				\item max\_iter: Максимальное количество итераций для одного запуска K-means.
				\item tol: Порог для остановки алгоритма. Если разница между старыми и новыми центроидами меньше этого значения, алгоритм останавливается.
				\item n\_init: Количество запусков алгоритма с различными инициализациями. Результат с наименьшей инерцией будет выбран как окончательный.
				\item random\_state: Начальное значение генератора случайных чисел для воспроизводимости результатов.
				\item algorithm: Алгоритм для вычисления K-means. Возможные значения включают 'auto', 'full', и 'elkan'.
			\end{itemize}
			
		\subsubsection{Пример использования}
			\begin{verbatim}
				kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10, random_state=0)

				# Обучение модели
				kmeans.fit(X)

				# Предсказание кластеров для данных
				y_kmeans = kmeans.predict(X)
			\end{verbatim}
			
			
			Пример преобразования данных:
				были точки, а теперь число которое показывет в каком кластере.
	  
	\subsection{DBSCAN}
		Алгоритм DBSCAN (Density-Based Spatial Clustering of Applications with Noise) рассматривает кластеры как области высокой плотности, отделенные от областей низкой плотности. Это позволяет DBSCAN обнаруживать кластеры любой формы, в отличие от K-means, который предполагает, что кластеры имеют выпуклую форму. Основной компонент DBSCAN - это понятие ядерных образцов, которые представляют собой образцы в областях высокой плотности.
		
		\subsubsection{Параметры}
			Парметры алгоритам DBSCAN:
			\begin{itemize}
				\item eps: Максимальное расстояние между двумя образцами, при котором один считается соседом другого. Это параметр радиуса для определения плотности области.
				\item min\_samples: Минимальное число образцов (включая сам образец), необходимое для того, чтобы область считалась плотной, т.е. чтобы образец стал ядерным.
				\item metric: Метрика для вычисления расстояний между точками (по умолчанию 'euclidean'). Это может быть любая метрика, поддерживаемая функцией scipy.spatial.distance.pdist.
				\item algorithm: Алгоритм, используемый для вычисления ближайших соседей ('auto', 'ball\_tree', 'kd\_tree', 'brute').
				\item leaf\_size: Размер листа, передаваемый алгоритмам 'ball\_tree', 'kd\_tree'. Это может повлиять на скорость построения и запроса ближайших соседей.
				\item p: Степень Минковского расстояния, когда используется метрика 'minkowski' (по умолчанию p=2, что соответствует евклидову расстоянию).	
			\end{itemize}
			
			
		\subsubsection{Описание алгоритма}
			Алгоритм DBSCAN реализуется следующим образом:
			\begin{enumerate}
				\item Определяются ядерные образцы: Образцы, для которых количество соседей в пределах eps не меньше min\_samples.
				\item Строятся кластеры: Для каждого ядерного образца строится кластер путем нахождения всех его соседей, которые также являются ядерными образцами, и их собственных соседей и так далее. Каждый кластер также включает не-ядерные образцы, которые являются соседями ядерных образцов в кластере.
				\item Выделяются выбросы: Все образцы, которые не являются ядерными и не находятся на достаточном расстоянии от ядерных образцов, считаются выбросами.
			\end{enumerate}
			Алгоритм DBSCAN детерминирован и всегда генерирует одинаковые кластеры при одинаковых входных данных в одном и том же порядке. Однако результаты могут отличаться при предоставлении данных в другом порядке. Это происходит из-за того, что порядок обработки данных влияет на то, каким образом назначаются образцы кластерам.
			
		\subsubsection{Пример использования}
			\begin{verbatim}
				X = np.array([[0, 1], [1, 1], [2, 1], [3, 1], [4, 1],
             			 [0, 0], [1, 0], [2, 0], [3, 0], [4, 0]])

					# Создание модели t-SNE
					tsne = TSNE(n_components=2, perplexity=5, n_iter=300, random_state=0)

					# Преобразование данных
					X_embedded = tsne.fit_transform(X)
			\end{verbatim}
			
			
			Пример преобразования данных:
				были точки, а теперь число которое показывет в каком кластере.
		
	\subsection{Выбор параметров}
			
	
	
\section{Визуализация}
	\subsection{Opis}
		Заключителый процесс анализа это визуализация данных. Итоговый результат представляет из себя \textbf{2D Plot}, где каждая точка будет представлять некоторые текстовые данные (комментарии) с опрелённым цветом, соотвутсвующим цвету кластера, которому она принадлежит, или же цвету отсутсвия кластера как в случае DBSCAN. Так же на графике будут визуализироваться центра каждого кластера в виде красного крестика. Для удобства навигации и проверки результатов  при наведении на точку(коментарий) или крустик(центр кластера) будет высчевиваться информация с содержанием элемента, которая репрезентуется с помощью библиотеки \textbf{mplcursor}. В случае точки это содержание обработоного комментария. В случае крестика репрезентуется центр кластера, которая с помощью метода обратного векторизации будет пытаться представить данную точку как слово.
	
	\subsection{Cнижение размерности}
		Для возможности визуализации векторизированых данных в качестве 2D точек необходимо воспользоваться методом снижения размерности (T-SNE). Данный метод является нелейным методом выделения признаков. Изначально трудно определить за что отвечают признаки, то выбрать какие-то наиболее подходящие признаки невозможно. Нелейный метод поможет нам сохранить неленейные связи.
		
	\subsection{T-SNE}
	
		\subsubsection{Параметры}
			\begin{itemize}
				\item n\_components: Количество измерений, в которое нужно проецировать данные (обычно 2 или 3 для визуализации).
				\item perplexity: Параметр, связанный с количеством ближайших соседей. Он влияет на баланс между локальной и глобальной структурой данных.
				\item early\_exaggeration: Параметр, который контролирует расстояния между точками в начальной фазе оптимизации.
				\item learning\_rate: Скорость обучения для градиентного спуска.
				\item n\_iter: Количество итераций для оптимизации.
				\item metric: Метрика для вычисления расстояний между точками (по умолчанию 'euclidean').
				\item init: Метод инициализации ('random' или 'pca').			
				\item random\_state: Начальное значение генератора случайных чисел для воспроизводимости результатов.
			\end{itemize}				
		
		\subsubsection{Описание алгоритма}
			Алгоритм t-SNE выполняется следующим образом:
			\begin{enumerate}
				\item Преобразование расстояний в вероятности: Для каждой точки рассчитываются вероятности Пирсона, которые определяют вероятность того, что соседняя точка будет выбрана в качестве соседа.
				\item Определение целевых вероятностей: На низкоразмерной проекции рассчитываются вероятности Q, аналогичные вероятностям Пирсона, но в другой размерности.
				\item  Минимизация расхождения (Kullback-Leibler divergence): Градиентный спуск используется для минимизации расхождения между распределениями P и Q, что приводит к сохранению близости точек.
			\end{enumerate}
			
			
		\subsubsection{Пример использования}
			\begin{verbatim}
				X = np.array([[1, 2], [2, 2], [2, 3],
              					[8, 7], [8, 8], [25, 80]])

				# Создание модели DBSCAN
				dbscan = DBSCAN(eps=3, min_samples=2)

				# Обучение модели
				dbscan.fit(X)

				# Предсказание кластеров для данных
				labels = dbscan.labels_
			\end{verbatim}
			
			
			Пример преобразования данных:
				\textbf{тут фотку}
	
	






