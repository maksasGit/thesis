\chapter{Rozdzial 1}

\section{Klasterzyacja}

	\subsection{Defenicja}
		Klasteryzacja (ang. clustering) to wielowymiarowa procedura analizy statystycznej, mająca na celu zbieranie danych zawierających informacje (cechy) o próbie obiektów, a następnie porządkowanie tych obiektów w stosunkowo jednorodne grupy. Zakres zastosowania klasteryzacji jest niezwykle szeroki: używa się jej w archeologii, medycynie, psychologii, chemii, biologii, administracji państwowej, filologii, antropologii, marketingu, socjologii, geologii i innych dyscyplinach. \cite{Clustering}
	
	\subsection{Cele klasteryzacji}
		Klasteryzacja jest najczęściej wykorzystywana do następujących celów:
		\begin{enumerate}
			\item Zrozumienie danych poprzez identyfikację struktury klastrów. Podział próby na grupy podobnych obiektów pozwala uprościć dalsze przetwarzanie danych i podejmowanie decyzji, stosując dla każdego klastra odpowiednią metodę analizy.
			\item Kompresja danych. Jeśli pierwotna próba jest zbyt duża, można ją skompresować, zachowując tylko jednego najbardziej typowego przedstawiciela z każdego klastra. Zastępując wszystkie obiekty w każdym klastrze jednym najbardziej typowym przedstawicielem, można znacznie zmniejszyć objętość danych, zachowując jednocześnie główne cechy i strukturę pierwotnych danych.
			\item Wykrywanie nowości (odchyleń lub anomalii). Klasteryzacja pozwala na wyróżnienie nietypowych obiektów, które nie mogą być przypisane do żadnego z klastrów. Te obiekty mogą być interesujące jako potencjalne anomalie lub odchylenia, wymagające dodatkowych badań, analiz lub uwagi.
		\end{enumerate}
	
	
	\subsection{Metody klasteryzacji}
		Nie ma powszechnie przyjętej klasyfikacji metod klasteryzacji, jednak można wyróżnić kilka grup podejść:

		\begin{enumerate}
			\item Algorytmy klasteryzacji oparte na grafach. Klasa ta obejmuje prymitywne algorytmy oparte na budowaniu grafu podobieństwa między obiektami. Obecnie są one praktycznie nie stosowane w praktyce.
			\item Algorytmy klasteryzacji probabilistycznej. Algorytmy te przypisują każdemu obiektowi z próby treningowej prawdopodobieństwo przynależności do każdego z klastrów.
			\item Hierarchiczne algorytmy klasteryzacji. Algorytmy te porządkują dane, tworząc hierarchię zagnieżdżonych klastrów.
			\item Algorytmy oparte na gęstości danych:
			\begin{itemize}
				\item K-średnich (K-means). Iteracyjny algorytm oparty na minimalizacji sumy kwadratowych odchyleń punktów klastrów od ich centrów.
				\item Rozpowszechnianie podobieństwa (Affinity Propagation). Algorytm, który rozprzestrzenia wiadomości o podobieństwie między parami obiektów w celu wyboru typowych przedstawicieli każdego klastra.
				\item Przesunięcie średniej (mean shift). Metoda wybierająca centroidy klastrów w obszarach o największej gęstości.
				\item Klasteryzacja spektralna (spectral clustering). Metoda wykorzystująca wartości własne macierzy odległości do zmniejszenia wymiarowości przed zastosowaniem innych metod klasteryzacji.
				\item DBSCAN (Density-Based Spatial Clustering of Applications with Noise). Algorytm grupujący punkty w jeden klaster w obszarach o wysokiej gęstości i oznaczający odosobnione punkty jako szum.
			\end{itemize}
		\end{enumerate}

		\subsection{Etapy klasteryzacji}
			Niezależnie od przedmiotu badań, zastosowanie analizy klastrów obejmuje następujące etapy:
			
			\begin{enumerate}
				\item Wybór próby do klasteryzacji. Zakłada się, że klasyfikować można tylko dane ilościowe, ponieważ wskaźniki liczbowe mogą być ilościowo oceniane.
				\item Określenie zestawu zmiennych, na podstawie których obiekty w próbie będą oceniane, tj. przestrzeni cech. Cechy mogą być różnymi charakterystykami obiektów, które mają znaczenie dla badań.
				\item Obliczanie wartości wybranej miary podobieństwa (lub różnicy) między obiektami. Celem jest określenie, jak blisko lub różnią się obiekty w próbie pod względem wybranych cech.
				\item Zastosowanie metody analizy klastrów do stworzenia grup podobnych obiektów. Zakłada się wykorzystanie jednego z istniejących algorytmów klasteryzacji w taki sposób, aby obiekty w jednym klastrze były maksymalnie podobne do siebie, a obiekty z różnych klastrów maksymalnie się różniły.
				\item Sprawdzenie wiarygodności wyników klasteryzacji. Może to być osiągnięte za pomocą różnych metod, takich jak wizualizacja klastrów, analiza stabilności klastrów i ocena jakości podziału.
			\end{enumerate}

	\subsection{Zastosowanie}
		Zastosowanie metody analizy klastrów jest z powodzeniem wykorzystywane w wielu dziedzinach i dyscyplinach. Metoda ta jest również szeroko stosowana w informatyce. Najbardziej udanymi przykładami wykorzystania klasteryzacji w pracy z danymi tekstowymi są:
			
			\begin{itemize}
				\item klasteryzacja wyników wyszukiwania
				\item grupowanie użytkowników o podobnych zainteresowaniach, co pomaga w personalizacji treści, rekomendacjach i targetowaniu w reklamie
				\item grupowanie tekstowych dokumentów według ich tematyki, takich jak artykuły prasowe, publikacje naukowe lub wiadomości w mediach społecznościowych
			\end{itemize}

	\subsection{Оценка эффективности кластеризации}
		Problem oceny jakości w zadaniu klasteryzacji jest trudny do rozwiązania z co najmniej dwóch powodów. Po pierwsze, zgodnie z twierdzeniem o niemożności Kleinberga \cite{Kleinberg}, nie istnieje optymalny algorytm klasteryzacji. Po drugie, wiele algorytmów klasteryzacji nie jest w stanie samodzielnie określić rzeczywistej liczby klastrów w danych; najczęściej liczba klastrów jest zadawana na wejściu algorytmu i dobierana kilkoma uruchomieniami.
		Niemniej jednak, wyróżnia się dwie grupy metod oceny jakości klasteryzacji:
		\begin{enumerate}
			\item Zewnętrzne metody porównują wynik klasteryzacji z apriorycznie znanym podziałem na klasy. Przykładem zewnętrznej metody oceny może być indeks Randa. Metoda ta ocenia, jak wiele spośród par elementów, które znajdowały się w jednej klasie, oraz tych, które znajdowały się w różnych klasach, zachowało ten stan po zastosowaniu algorytmu klasteryzacji.
			\item Wewnętrzne metody oceniają jakość klasteryzacji, korzystając jedynie z informacji zawartych w samych danych. Przykładem wewnętrznej metody jest metoda Spójności Klastra (Cluster Cohesion). Idea tej metody polega na tym, że im bliżej siebie znajdują się obiekty w klastrach, tym lepsze jest podzielenie. Można również wyróżnić metodę Separacji Klastra (Cluster Separation), która ocenia jakość klasteryzacji na podstawie odległości między obiektami różnych klastrów.
		\end{enumerate}


\section{Обработка естественного языка}

	\subsection{Opis}
		Обработка естественного языка (Natural Language Processing, NLP) — это область исследований, которая объединяет методы машинного обучения и математической лингвистики с целью изучения и создания алгоритмов для анализа, понимания и генерации текста на естественных языках. \cite{NLP}


	\subsection{Предобработка текста}
		Предобработка текста является важным этапом NLP, которая переводит текст на естественном языке в формат удобный для распознования алгоритмами и дальнейшей работы. Предобработка состоит из различных этапов, которые могут отличаться в зависимости от задачи и реализации. Далее приведены одни из самых популярных основных подходов:
	\begin{itemize}
		\item Перевод всех букв в тексте в нижний или верхний регистры
		\item Удаление цифр (чисел) или замена на текстовый эквивалент (обычно используются регулярные выражения)
		\item Удаление пробельных символов (whitespaces)
		\item Токенизация (обычно реализуется на основе регулярных выражений)
		\item Удаление стоп слов
		\item Стемминг
		\item Лемматизация
		\item Векторизация
	\end{itemize}
	Далее подробнее будут описаны три последних метода.

	\subsection{Стемминг}
		Количество корректных словоформ, значения которых схожи, но написания отличаются суффиксами, приставками, окончаниями и прочим, очень велико, что усложняет создание словарей и дальнейшую обработку. Стемминг позволяет привести слово к его основной форме. Суть подхода в нахождении основы слова, для этого с конца и начала слова последовательно отрезаются его части. Правила отсекания для стеммера создаются заранее, и чаще всего представляют из себя регулярные выражения, что делает данный подход трудоемким, так как при подключении очередного языка нужны новые лингвистические исследования. Вторым недостатком подхода является возможная потеря информации при отрезании частей, например, мы можем потерять информацию о части речи.

	\subsection{Лемматизация}
		Данный подход является альтернативой стемминга. Основная идея в приведении слова к словарной форме — лемме. 				Пример лемматиации для польского языка:
		\begin{itemize}
			\item для существительных — именительный падеж, единственное число;
			\item для прилагательных — именительный падеж, единственное число, мужской род;
			\item для глаголов, причастий, деепричастий — глагол в инфинитиве несовершенного вида.
		\end{itemize}
		

\section{Векторизация}

	\subsection{Opis}
		Большинство математических моделей работают в векторных пространствах больших размерностей, поэтому необходимо отобразить текст в векторном пространстве. Основной целью векторизации текста является создание представления текстовых данных, которое сохраняет семантическую и синтаксическую информацию о тексте и одновременно подходит для использования в математических моделях. Это позволяет эффективно анализировать и обрабатывать текстовые данные с помощью алгоритмов и алгоритмов машинного обучения. \cite{Vectorization}

	\subsection{Задачи векторизации}
		Основные задачи векторизации текста включают:
		\begin{itemize}
			\item Представление текста для анализа машинными алгоритмами.
			\item Поиск схожести между текстами или категоризация текстов по темам.
			\item Решение задач классификации, кластеризации или регрессии на основе текстовых данных.
			\item Создание моделей машинного обучения для автоматического обобщения текстовой информации
		\end{itemize}

	\subsection{Методы векторизации}
		Первый шаг в векторизации текста - это определение пространства признаков, в котором текст будет представлен в виде векторов. Каждый признак может представлять собой слово, фразу, символ или иной элемент текста.\\
		Существует несколько методов векторизации текста, включая:
		\begin{itemize}
			\item Мешок слов (Bag of Words, BoW): Каждый документ представляется в виде вектора, где каждый элемент соответствует отдельному слову, а значениями являются частоты встречаемости слов в документе.
			\item TF-IDF (Term Frequency-Inverse Document Frequency): Этот метод учитывает не только частоту встречаемости слов в документе (TF), но и инверсную частоту встречаемости слова во всех документах корпуса (IDF), что позволяет выделить ключевые слова.
			\item Word Embeddings: Это методы, основанные на обучении нейронных сетей, которые преобразуют слова в векторы непрерывного пространства с сохранением их семантических свойств.\\
		\end{itemize}

	\subsection{Применение}
		Векторизация текста находит применение во многих областях, включая:
		\begin{itemize}
			\item Анализ тональности и сентиментов в социальных медиа и отзывах
			\item Классификация текстов на тематику или категории
			\item Рекомендательные системы, основанные на текстовых описаниях
			\item Автоматический перевод текста и реализация чат-ботов.
		\end{itemize}


\section{снижения разменрности}

	\subsection{Дефениция}
		Снижение размерности - это процесс уменьшения количества признаков (измерений) в наборе данных, сохраняя при этом как можно больше информации. Часто используется в машином обучении где главная цель устранение в наборе  признаков избыточных , неинформативных или слабо информативных которые могут  понизить эффективность модели, а после такого преобразования она упрощается, и соответственно уменьшается размер набора данных в памяти и ускоряется работа алгоритмов на нем. Так же используется для упрощение анализа и визуализации данных, например путем редукции до низких размерностей таких как 2D или 3D. \cite{DimensionReduction}

	\subsection{Методы снижения размерности}
		Уменьшение размерности может быть осуществлено методами выбора признаков (англ. feature selection) или выделения признаков (англ. feature extraction).
		
		\subsubsection{Выбор признаков}
			Метод Выбор признаков оставляют некоторое подмножество исходного набора признаков, избавляясь от признаков избыточных и слабо информативных. Основные преимущества этого класса алгоритмов:
			\begin{itemize}
				\item Уменьшение вероятности переобучения
				\item Увеличение точности предсказания модели;
				\item Сокращение времени обучения;
				\item Увеличивается семантическое понимание модели.\\
			\end{itemize}
		
		\subsubsection{Выделение признаков}	
			Другим способом уменьшить размерность входных данных является выделение признаков. Эти методы каким-то образом составляют из уже исходных признаков новые, все также полностью описывающие пространство набора данных, но уменьшая его размерность и теряя в репрезентативности данных, т.к. становится непонятно, за что отвечают новые признаки. Все методы feature extraction можно разделить на линейные и нелинейные.\\

		\subsubsection{Методы выделения признаков}
			Линейные методы основываются на предположение, что зависимость между независимыми переменными и зависимой переменной линейна. Поэтому они с помощью линейых комбинацией, которые означают, что изменение в одной переменной приводит к пропорциональному изменению в другой. Одним из самых известных методов линейного выделения признаков является PCA . Основной идеей этого метода является поиск  гиперплоскости, на которую при ортогональной проекции всех признаков максимизируется дисперсия.
			Нелинейные методы позволяют моделировать более сложные и нелинейные зависимости между переменными. Алгоритм t-sne стремится сохранить относительные расстояния между точками в исходном пространстве высокой размерности, а затем  пытается воссоздать это распределение в низкоразмерном пространстве.

	\subsection{Применение}
		Метод снижение размерности часто и успешно используется в Машином обучение , Обработка изображений и компьютерное зрение, Биоинформатика и геномика , Финансовый анализ и торговля и многие другие области. Где снижение количества признаков позволяет оптимизировать обработку данных.



