\chapter{Rozdzial 1}

\section{Klasterzyacja}

	\subsection{Defenicja}
		Кластеризация представляет собой многомерную процедуру статистического анализа, направленную на сбор данных, содержащих информацию (признаки) о выборке объектов, с последующим упорядочиванием этих объектов в относительно однородные группы. Область применения кластеризации чрезвычайно широка: её используют в археологии, медицине, психологии, химии, биологии, государственном управлении, филологии, антропологии, маркетинге, социологии, геологии и других дисциплинах.
	
	\subsection{Цели кластеризации}
		Кластеризация чаще всего используется для следующих целей:
		\begin{enumerate}
			\item Понимание данных путем идентификации структуры кластеров. Разделение выборки на группы похожих 	объектов позволяет упростить дальнейшую обработку данных и принятие решений, применяя для каждого кластера свой метод анализа.
			\item Компрессия данных. Если исходная выборка слишком велика, её можно сжать, сохраняя только по одному наиболее типичному представителю из каждого кластера. Заменяя все объекты в каждом кластере одним наиболее типичным представителем, можно значительно снизить объём данных, сохраняя при этом основные характеристики и структуру исходных данных.
			\item Выявление новизны (выбросов или аномалий). Кластеризация позволяет выделить нетипичные объекты, которые нельзя отнести к какому-либо из кластеров. Эти объекты могут представлять интерес как потенциальные аномалии или выбросы, требующие дополнительного изучения, исследования или внимания.
		\end{enumerate}
	
	
	\subsection{Методы кластеризации}
		Общепринятой классификации методов кластеризации не существует, однако можно выделить ряд групп подходов:
		\begin{enumerate}
			\item Графовые алгоритмы кластеризации. Этот класс включает примитивные алгоритмы, основанные на построении графа сходства между объектами. В настоящее время они практически не применяются на практике.
			\item Вероятностные алгоритмы кластеризации. Эти алгоритмы присваивают каждому объекту из обучающей выборки вероятность принадлежности к каждому из кластеров.
			\item Иерархические алгоритмы кластеризации. Эти алгоритмы упорядочивают данные, создавая иерархию вложенных кластеров.
			\item Алгоритмы, основанные на плотности данных:
			\begin{itemize}
				\item K-средних. Итеративный алгоритм, основанный на минимизации суммарного квадратичного отклонения точек кластеров от их центров.
				\item Распространение похожести. Алгоритм, который распространяет сообщения о сходстве между парами объектов для выбора типичных представителей каждого кластера.
				\item Сдвиг среднего значения (mean shift). Метод, выбирающий центроиды кластеров в областях с наибольшей плотностью.
				\item Спектральная кластеризация. Метод, использующий собственные значения матрицы расстояний для понижения размерности перед применением других методов кластеризации.
				\item DBSCAN (Density-Based Spatial Clustering of Applications with Noise). Алгоритм, группирующий точки в один кластер в областях с высокой плотностью и помечающий одиноко расположенные точки как шум.
			\end{itemize}
		\end{enumerate}

		\subsection{Этапы кластеризации}
			Независимо от предмета исследования применение анализа кластеров включает следующие этапы:
			\begin{enumerate}
				\item Отбор выборки для кластеризации. Предполагается, что разумно классифицировать только количественные данные, поскольку числовые показатели могут быть количественно оценены.
				\item Определение набора переменных, на основе которых объекты в выборке будут оцениваться, то есть пространства признаков. Признаки могут быть различными характеристиками объектов, которые имеют значение для исследования.
				\item Вычисление значений той или иной меры сходства (или различия) между объектами. Цель - определить, насколько близки или различны объекты в выборке с точки зрения выбранных признаков.
				\item Применение метода кластерного анализа для создания групп схожих объектов. Подруземевает использование одного из существуещего алгоритма кластеризации таким образом, чтобы объекты внутри одного кластера были максимально похожи друг на друга, а объекты из разных кластеров - максимально отличались.
				\item Проверка достоверности результатов кластерного решения. Это может быть достигнуто с помощью различных методов, таких как визуализация кластеров, анализ стабильности кластеров и проверка качества разделения.
			\end{enumerate}

	\subsection{Применение}
			Применение метода кластерного анализа успешно используется во многих областях и дисциплинах. Также данный метод широко используется в информатике. Наиболее успешными примерами использования кластеризации в работе с текстовыми данными являются: 
			\begin{itemize}
				\item кластеризация результатов поисковых запросов
				\item группировка пользователей со схожими интересами, которая помогает в персонализации контента, рекомендациях и таргетинге в рекламе
				\item группировка текстовых документов по их тематике, таких как новостные статьи, научные публикации или сообщения в социальных сетях
			\end{itemize}

	\subsection{Оценка эффективности кластеризации}
		Проблема оценки качества в задаче кластеризации трудноразрешима, как минимум, по двум причинам. Во-первых, согласно теореме невозможности Клейнберга \cite{Kleinberg}, не существует оптимального алгоритма кластеризации. Во-вторых, многие алгоритмы кластеризации не способны самостоятельно определить настоящее количество кластеров в данных; чаще всего количество кластеров задаётся на входе алгоритма и подбирается несколькими запусками.
		Тем не менее, принято выделять две группы методов оценки качества кластеризации:
		\begin{enumerate}
			\item Внешние методы сравнивают результат кластеризации с априори известным разделением на классы. Примером внешнего метода оценки можно рассмотреть индекс Рэнда. Этот метод оценивает, насколько много из тех пар элементов, которые находились в одном классе, и тех пар элементов, которые находились в разных классах, сохранили это состояние после применения алгоритма кластеризации.
			\item Внутренние методы оценивают качество кластеризации, используя только информацию, содержащуюся в самих данных. Примером внутреннего метода является метод Cluster Cohesion. Идея данного метода заключается в том, что чем ближе друг к другу находятся объекты внутри кластеров, тем лучше разделение. Также можно отметить метод Cluster Separation, который оценивает качество кластеризации по тому, насколько далеко друг от друга находятся объекты разных кластеров.
		\end{enumerate}


\section{Обработка естественного языка}

	\subsection{Opis}
		Обработка естественного языка (Natural Language Processing, NLP) — это область исследований, которая объединяет методы машинного обучения и математической лингвистики с целью изучения и создания алгоритмов для анализа, понимания и генерации текста на естественных языках.


	\subsection{Предобработка текста}
		Предобработка текста является важным этапом NLP, которая переводит текст на естественном языке в формат удобный для распознования алгоритмами и дальнейшей работы. Предобработка состоит из различных этапов, которые могут отличаться в зависимости от задачи и реализации. Далее приведены одни из самых популярных основных подходов:
	\begin{itemize}
		\item Перевод всех букв в тексте в нижний или верхний регистры
		\item Удаление цифр (чисел) или замена на текстовый эквивалент (обычно используются регулярные выражения)
		\item Удаление пробельных символов (whitespaces)
		\item Токенизация (обычно реализуется на основе регулярных выражений)
		\item Удаление стоп слов
		\item Стемминг
		\item Лемматизация
		\item Векторизация
	\end{itemize}
	Далее подробнее будут описаны три последних метода.

	\subsection{Стемминг}
		Количество корректных словоформ, значения которых схожи, но написания отличаются суффиксами, приставками, окончаниями и прочим, очень велико, что усложняет создание словарей и дальнейшую обработку. Стемминг позволяет привести слово к его основной форме. Суть подхода в нахождении основы слова, для этого с конца и начала слова последовательно отрезаются его части. Правила отсекания для стеммера создаются заранее, и чаще всего представляют из себя регулярные выражения, что делает данный подход трудоемким, так как при подключении очередного языка нужны новые лингвистические исследования. Вторым недостатком подхода является возможная потеря информации при отрезании частей, например, мы можем потерять информацию о части речи.

	\subsection{Лемматизация}
		Данный подход является альтернативой стемминга. Основная идея в приведении слова к словарной форме — лемме. 				Пример лемматиации для польского языка:
		\begin{itemize}
			\item для существительных — именительный падеж, единственное число;
			\item для прилагательных — именительный падеж, единственное число, мужской род;
			\item для глаголов, причастий, деепричастий — глагол в инфинитиве несовершенного вида.
		\end{itemize}
		

\section{Векторизация}

	\subsection{Opis}
		Большинство математических моделей работают в векторных пространствах больших размерностей, поэтому необходимо отобразить текст в векторном пространстве. Основной целью векторизации текста является создание представления текстовых данных, которое сохраняет семантическую и синтаксическую информацию о тексте и одновременно подходит для использования в математических моделях. Это позволяет эффективно анализировать и обрабатывать текстовые данные с помощью алгоритмов и алгоритмов машинного обучения.

	\subsection{Задачи векторизации}
		Основные задачи векторизации текста включают:
		\begin{itemize}
			\item Представление текста для анализа машинными алгоритмами.
			\item Поиск схожести между текстами или категоризация текстов по темам.
			\item Решение задач классификации, кластеризации или регрессии на основе текстовых данных.
			\item Создание моделей машинного обучения для автоматического обобщения текстовой информации
		\end{itemize}

	\subsection{Методы векторизации}
		Первый шаг в векторизации текста - это определение пространства признаков, в котором текст будет представлен в виде векторов. Каждый признак может представлять собой слово, фразу, символ или иной элемент текста.\\
		Существует несколько методов векторизации текста, включая:
		\begin{itemize}
			\item Мешок слов (Bag of Words, BoW): Каждый документ представляется в виде вектора, где каждый элемент соответствует отдельному слову, а значениями являются частоты встречаемости слов в документе.
			\item TF-IDF (Term Frequency-Inverse Document Frequency): Этот метод учитывает не только частоту встречаемости слов в документе (TF), но и инверсную частоту встречаемости слова во всех документах корпуса (IDF), что позволяет выделить ключевые слова.
			\item Word Embeddings: Это методы, основанные на обучении нейронных сетей, которые преобразуют слова в векторы непрерывного пространства с сохранением их семантических свойств.\\
		\end{itemize}

	\subsection{Применение}
		Векторизация текста находит применение во многих областях, включая:
		\begin{itemize}
			\item Анализ тональности и сентиментов в социальных медиа и отзывах
			\item Классификация текстов на тематику или категории
			\item Рекомендательные системы, основанные на текстовых описаниях
			\item Автоматический перевод текста и реализация чат-ботов.
		\end{itemize}


\section{снижения разменрности}

	\subsection{Дефениция}
		Снижение размерности - это процесс уменьшения количества признаков (измерений) в наборе данных, сохраняя при этом как можно больше информации. Часто используется в машином обучении где главная цель устранение в наборе  признаков избыточных , неинформативных или слабо информативных которые могут  понизить эффективность модели, а после такого преобразования она упрощается, и соответственно уменьшается размер набора данных в памяти и ускоряется работа алгоритмов на нем. Так же используется для упрощение анализа и визуализации данных, например путем редукции до низких размерностей таких как 2D или 3D.

	\subsection{Методы снижения размерности}
		Уменьшение размерности может быть осуществлено методами выбора признаков (англ. feature selection) или выделения признаков (англ. feature extraction).
		
		\subsubsection{Выбор признаков}
			Метод Выбор признаков оставляют некоторое подмножество исходного набора признаков, избавляясь от признаков избыточных и слабо информативных. Основные преимущества этого класса алгоритмов:
			\begin{itemize}
				\item Уменьшение вероятности переобучения
				\item Увеличение точности предсказания модели;
				\item Сокращение времени обучения;
				\item Увеличивается семантическое понимание модели.\\
			\end{itemize}
		
		\subsubsection{Выделение признаков}	
			Другим способом уменьшить размерность входных данных является выделение признаков. Эти методы каким-то образом составляют из уже исходных признаков новые, все также полностью описывающие пространство набора данных, но уменьшая его размерность и теряя в репрезентативности данных, т.к. становится непонятно, за что отвечают новые признаки. Все методы feature extraction можно разделить на линейные и нелинейные.\\

		\subsubsection{Методы выделения признаков}
			Линейные методы основываются на предположение, что зависимость между независимыми переменными и зависимой переменной линейна. Поэтому они с помощью линейых комбинацией, которые означают, что изменение в одной переменной приводит к пропорциональному изменению в другой. Одним из самых известных методов линейного выделения признаков является PCA . Основной идеей этого метода является поиск  гиперплоскости, на которую при ортогональной проекции всех признаков максимизируется дисперсия.
			Нелинейные методы позволяют моделировать более сложные и нелинейные зависимости между переменными. Алгоритм t-sne стремится сохранить относительные расстояния между точками в исходном пространстве высокой размерности, а затем  пытается воссоздать это распределение в низкоразмерном пространстве.

	\subsection{Применение}
		Метод снижение размерности часто и успешно используется в Машином обучение , Обработка изображений и компьютерное зрение, Биоинформатика и геномика , Финансовый анализ и торговля и многие другие области. Где снижение количества признаков позволяет оптимизировать обработку данных.



